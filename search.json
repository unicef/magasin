[
  {
    "objectID": "admin-guides/superset.html",
    "href": "admin-guides/superset.html",
    "title": "Superset guide",
    "section": "",
    "text": "A quick guide for an magasin administrator on superset."
  },
  {
    "objectID": "admin-guides/superset.html#backup-the-superset-database-into-the-local-filesystem",
    "href": "admin-guides/superset.html#backup-the-superset-database-into-the-local-filesystem",
    "title": "Superset guide",
    "section": "1 Backup the superset database into the local filesystem",
    "text": "1 Backup the superset database into the local filesystem\nA dump (backup) of the database may be useful when a new version of superset is going to be installed, or to keep a regular scheduled backup of the superset data.\nTo create a dump of the database, you can use the script scripts/superset/dump-superset-db.sh:\n./dump-superset-db.sh -n magasin-superset-prd`\nWhere the option -n is to specify the namespace of the superset instance (defaults to magasin-superset).\nThis script will request request the password of the superset user, to perform the dump. You can get the password from the secret superset-env within the same namespace.\nThe output of running this script is the file superset_dump.sql in the current working folder. Note that this script assumes that the name of the database is superset.\nAlternatively, these are the manual steps to do the same:\n\nLaunch a terminal in the posgresql pod (replace the namespace if required)\n kubectl exec superset-postgresql-0 -ti --namespace magasin-superset -- /bin/bash\nWithin the pod shell, extract and compress the data. The below command will ask for the superset database password (you can find it in the superset-env secret, defaults to superset)\npg_dump -U superset -d superset &gt; /tmp/superset_dump.sql\n# where -U is the username of the database, and -d is the database name.\n\ntar -C /tmp -czvf /tmp/supserset_dump.tgz supserset_dump.sql\n# where -C is to use /tmp as base folder -c collect, z zip, v=verboze and f output file\nExit the pod shell and copy the dump file into /tmp in the local filesystem of the pod. exit\n# Exit the pod shell\nexit\n\n# Copy the tgz to the local environment.\nkubectl cp magasin-superset/superset-postgresql-0:/tmp/superset_dump.tgz \\\n./supserset_dump.tgz --retries 100\n\n# Syntax: kubectl cp namespace/pod:path local-path. \n# The option --retries is to fix potential network issues\nLastly, delete the db dump from the postgres pod\nkubectl exec magasin-superset/superset-postgresql-0 -- \\\nrm /tmp/supeset_dump.tgz /tmp/superset_dump.sql  \n\n\n1.1 Restore superset database from local filesystem\nTo restore the database from the previous step.\n\nCopy the database to the postgres pod.\nkubectl cp superset_dump.sql magasin-superset/superset-postgresql-0:/tmp/superset_dump.sql\nLaunch the shell\nkubectl exec superset-postgresql-0 -ti --namespace magasin-superset -- /bin/bash\nIf the database exist, you may need to drop it first\npsql --username postgres\nThe password can be found in superset-postgresql secret postgres-password\nIf you get an error indicating the database is being used run the command:\n\nSELECT pg_terminate_backend(pg_stat_activity.pid) \n    FROM pg_stat_activity \n    WHERE pg_stat_activity.datname = 'superset' AND pid &lt;&gt; pg_backend_pid();\n\n-- To delete the superset database: (don't forget the ; at the end)\nDROP DATABASE superset;\n\n--- Then create an empty database (don't forget the ; at the end)\nCREATE DATABASE superset;\n\n-- Then quit psql\n\\q\nFinally, restore the database\npsql --username superset --dbname superset -f /tmp/superset_dump.sql\nOr alternatively less verbose option\npsql -U superset -d superset2 -f ./tmp/superset_dump.sql\n\nNow, you can restart the superset pod. First find the superset-xxxxxxxxx-xxxxx and delete it\nkubectl get pods --namespace magasin-superset\n\nNAME                               READY   STATUS      RESTARTS   AGE\nsuperset-75b79c6c8d-jttd9          1/1     Running     0          62m\nsuperset-init-db-bhc25             0/1     Completed   0          62m\nsuperset-postgresql-0              1/1     Running     0          62m\nsuperset-redis-master-0            1/1     Running     0          62m\nsuperset-worker-85dfbb48dd-6bhjp   1/1     Running     0          62m\nkubectl delete pod superset-75b79c6c8d-jttd9"
  },
  {
    "objectID": "admin-guides/superset.html#postgresql",
    "href": "admin-guides/superset.html#postgresql",
    "title": "Superset guide",
    "section": "2 PostgreSQL",
    "text": "2 PostgreSQL\n\nGet list of databases\nSELECT datname FROM pg_database WHERE datistemplate = false;\nGet the user owner of database ‘database_name’\nSELECT d.datname as \"Name\",\npg_catalog.pg_get_userbyid(d.datdba) as \"Owner\"\nFROM pg_catalog.pg_database d\nWHERE d.datname = 'database_name'\nORDER BY 1;"
  },
  {
    "objectID": "admin-guides/superset.html#troubleshooting",
    "href": "admin-guides/superset.html#troubleshooting",
    "title": "Superset guide",
    "section": "3 Troubleshooting",
    "text": "3 Troubleshooting\n\nGet the events of a pod\nkubectl describe pod superset-564564dd4-2lzb6 --namespace magasin-superset\nGet logs of the containers\nkubectl logs &lt;podname&gt; -f --namespace &lt;namespace&gt; \nExample:\nkubectl logs superset-564564dd4-2lzb6 -f --namespace magasin-superset\nwhere -f continues displaying the new logs.\nWith --all-containers you can see the logs of all the containers instead of the one that is being executed. This is useful when the current container of the pod gets stuck because of the previous container output was not the expected. Example:\nkubectl logs superset-564564dd4-2lzb6 --namespace magasin-superset --all-containers"
  },
  {
    "objectID": "admin-guides/drill.html",
    "href": "admin-guides/drill.html",
    "title": "Drill guide",
    "section": "",
    "text": "A quick guide for managing magasin’s Apache Drill instance."
  },
  {
    "objectID": "admin-guides/drill.html#apache-drill-options-and-tuning",
    "href": "admin-guides/drill.html#apache-drill-options-and-tuning",
    "title": "Drill guide",
    "section": "1 Apache Drill options and tuning",
    "text": "1 Apache Drill options and tuning\nThe following option values are recommended for Apache Drill’s use in the Magasin context:\nplanner.width.max_per_node = 3\nplanner.width.max_per_query = 12\nstore.parquet.reader.int96_as_timestamp = true\ndrill.exec.http.rest.errors.verbose = true\nexec.errors.verbose = true\nexec.queue.enable = true\nexec.queue.large = 2\nexec.queue.small = 10"
  },
  {
    "objectID": "admin-guides/drill.html#example-apache-drill-storage-plugin-for-azure-blob-storage",
    "href": "admin-guides/drill.html#example-apache-drill-storage-plugin-for-azure-blob-storage",
    "title": "Drill guide",
    "section": "2 Example Apache Drill storage plugin for Azure blob storage",
    "text": "2 Example Apache Drill storage plugin for Azure blob storage\nExample Apache Drill storage plugin configuration for a specific Azure blob storage container (unicef-magasin-dev) in a given Azure blob storage account (sauniwebsaksio).\nThe Apache Drill Azure Blob Storage Plugin must be present in $DRILL_HOME/jars/3rdparty. Note the Azure authentication key has been redacted in the below example.\nThis configuration is read-only and specifies support for certain file formats within two directory locations (/profiles and /datasets) within the storage container.\nFor production configuration, file formats should be restricted to the minimal set of expected formats. Multiple storage plugin instances can be configured in a single Apache Drill instance so separate Azure blob storage accounts and containers may be separately configured and queried.\n{\n  \"name\" : \"storage_account_blob_container\",\n  \"config\" : {\n    \"type\" : \"file\",\n    \"connection\" : \"wasbs://blob_container@storage_account.blob.core.windows.net\",\n    \"config\" : {\n      \"fs.azure.account.key.storage_account.blob.core.windows.net\" : \"*******\"\n    },\n    \"workspaces\" : {\n      \"profiles\" : {\n        \"location\" : \"/profiles\",\n        \"writable\" : false,\n        \"defaultInputFormat\" : null,\n        \"allowAccessOutsideWorkspace\" : false\n      },\n      \"datasets\" : {\n        \"location\" : \"/datasets\",\n        \"writable\" : false,\n        \"defaultInputFormat\" : null,\n        \"allowAccessOutsideWorkspace\" : false\n      }\n    },\n    \"formats\" : {\n      \"image\" : {\n        \"type\" : \"image\",\n        \"extensions\" : [ \"jpg\", \"jpeg\", \"jpe\", \"tif\", \"tiff\", \"dng\", \"psd\", \"png\", \"bmp\", \"gif\", \"ico\", \"pcx\", \"wav\", \"wave\", \"avi\", \"webp\", \"mov\", \"mp4\", \"m4a\", \"m4p\", \"m4b\", \"m4r\", \"m4v\", \"3gp\", \"3g2\", \"eps\", \"epsf\", \"epsi\", \"ai\", \"arw\", \"crw\", \"cr2\", \"nef\", \"orf\", \"raf\", \"rw2\", \"rwl\", \"srw\", \"x3f\" ],\n        \"fileSystemMetadata\" : true,\n        \"descriptive\" : true\n      },\n      \"parquet\" : {\n        \"type\" : \"parquet\"\n      },\n      \"avro\" : {\n        \"type\" : \"avro\",\n        \"extensions\" : [ \"avro\" ]\n      },\n      \"json\" : {\n        \"type\" : \"json\",\n        \"extensions\" : [ \"json\" ]\n      },\n      \"sequencefile\" : {\n        \"type\" : \"sequencefile\",\n        \"extensions\" : [ \"seq\" ]\n      },\n      \"tsv\" : {\n        \"type\" : \"text\",\n        \"extensions\" : [ \"tsv\" ],\n        \"fieldDelimiter\" : \"\\t\"\n      },\n      \"csvh\" : {\n        \"type\" : \"text\",\n        \"extensions\" : [ \"csvh\" ],\n        \"extractHeader\" : true\n      },\n      \"csv\" : {\n        \"type\" : \"text\",\n        \"extensions\" : [ \"csv\" ]\n      },\n      \"psv\" : {\n        \"type\" : \"text\",\n        \"extensions\" : [ \"tbl\" ],\n        \"fieldDelimiter\" : \"|\"\n      },\n      \"pcap\" : {\n        \"type\" : \"pcap\",\n        \"extensions\" : [ \"pcap\" ]\n      },\n      \"httpd\" : {\n        \"type\" : \"httpd\",\n        \"extensions\" : [ \"httpd\" ],\n        \"logFormat\" : \"%h %t \\\"%r\\\" %&gt;s %b \\\"%{Referer}i\\\"\"\n      }\n    },\n    \"enabled\" : true\n  }\n}"
  },
  {
    "objectID": "admin-guides/drill.html#backup-the-drill-storage-accounts-database",
    "href": "admin-guides/drill.html#backup-the-drill-storage-accounts-database",
    "title": "Drill guide",
    "section": "3 Backup the drill storage accounts database",
    "text": "3 Backup the drill storage accounts database\nApache Drill keeps a set of connections to storage accounts (S3 buckets, azure blobs, MinIO accounts…). The storage accounts is where the actual data is stored.\nHowever, in magasin given that Drill is setup in a distributed mode, the configuration of these connections is kept in zookeeper which is used to sync the storage configuration among the different instances of Drill. So, to backup the storage accounts we need to backup the zookeeper database which is a plain database.\n\nLaunch a terminal within the zookeeper pod (zk-0).\nkubectl exec zk-0 --namespace magasin-drill -ti -- /bin/bash\nCompress the files using tar, and leave the shell\ntar -czvf /tmp/zk.tgz /var/lib/zookeeper/*\nexit\nSave the file into our local filesystem\nkubectl cp magasin-drill/zk-0:/tmp/zk.tgz ./zk.tgz\nDelete the .tgz file of the pod\nkubectl exec zk-0 --namespace magasin-drill -- rm tmp/zk.tgz"
  },
  {
    "objectID": "admin-guides/drill.html#restore-the-drill-storage-configuration",
    "href": "admin-guides/drill.html#restore-the-drill-storage-configuration",
    "title": "Drill guide",
    "section": "4 Restore the Drill storage configuration",
    "text": "4 Restore the Drill storage configuration\nTo restore the backup created in the previous section:\n\nCopy the backup to the cluster\n kubectl cp ./zk.tgz magasin-drill/zk-0:/tmp/zk.tgz \nLaunch the shell in the zookeeper pod\nkubectl exec zk-0 --namespace magasin-drill -ti -- /bin/bash\nUnzip\ntar -xzvf /tmp/zk.tgz \nKill the java process\nps -ax \n\nPID TTY      STAT   TIME COMMAND\n1 ?          Ss     0:00 /rosetta/rosetta /usr/bin/bash /usr/bin/start.sh\n25 ?         Sl     0:03 /rosetta/rosetta /usr/bin/java -Dzookeeper.log.dir=/opt/zookeeper/bin/../logs -Dzookeeper.log.file=zookeeper-\n116 pts/0    Ss     0:00 /rosetta/rosetta /bin/bash\n148 ?        S      0:00 /rosetta/rosetta /usr/bin/sleep 5\n149 pts/0    R+     0:00 /usr/bin/ps -ax\n\n# Where -9 is to force kill and 25 is the process number (PID)\nkill -9 25\nRelaunch the initalizer script in background (&), then exit\n/usr/bin/start.sh &\nexit"
  },
  {
    "objectID": "admin-guides/daskhub.html",
    "href": "admin-guides/daskhub.html",
    "title": "Daskhub Admin Guide",
    "section": "",
    "text": "Because the default setup is only for testing purposes, by default dask’s Jupyterhub comes without authentication.\nTo setup the authentication mechanism you have all the documentation in: https://z2jh.jupyter.org/en/stable/administrator/authentication.html\nOnce you have created the yaml file that has the setup you can run helm upgrade\n# helm upgrade &lt;component&gt; --namespace &lt;component-namespace&gt; --values &lt;values-file.yaml&gt;\nhelm upgrade daskhub --namespace magasin-daskhub --values daskhub.yaml"
  },
  {
    "objectID": "admin-guides/daskhub.html#authentication",
    "href": "admin-guides/daskhub.html#authentication",
    "title": "Daskhub Admin Guide",
    "section": "",
    "text": "Because the default setup is only for testing purposes, by default dask’s Jupyterhub comes without authentication.\nTo setup the authentication mechanism you have all the documentation in: https://z2jh.jupyter.org/en/stable/administrator/authentication.html\nOnce you have created the yaml file that has the setup you can run helm upgrade\n# helm upgrade &lt;component&gt; --namespace &lt;component-namespace&gt; --values &lt;values-file.yaml&gt;\nhelm upgrade daskhub --namespace magasin-daskhub --values daskhub.yaml"
  },
  {
    "objectID": "install/index.html",
    "href": "install/index.html",
    "title": "Installation",
    "section": "",
    "text": "Magasin is a scalable end-to-end data platform based on open-source components that are natively run in a kubernetes cluster.\nKubernetes is a container orchestration system designed to automate the deployment, scaling, and management of containerized applications. It is an integral part of services offered by major cloud providers. Kubernetes, being open source, can also be set up on-premises. For testing purposes, it is even possible to install it on a desktop computer.\nMagasin uses Kubernetes in combination with Helm, a package manager for Kubernetes applications. Helm is the equivalent to apt, pip, npm, pacman, snap, conda. Using Helm, users specify the configuration of required Kubernetes resources to deploy magasin through a values file or command-line overrides. A package in helm is called chart.\nMagasin includes an installer script that sets up all the necessary dependencies on your computer, enabling the seamless setup of the magasin within the Kubernetes cluster.",
    "crumbs": [
      "Home",
      "Install",
      "Installation"
    ]
  },
  {
    "objectID": "install/index.html#deploy-magasin-using-the-installer-beta",
    "href": "install/index.html#deploy-magasin-using-the-installer-beta",
    "title": "Installation",
    "section": "1 Deploy magasin using the installer (beta)",
    "text": "1 Deploy magasin using the installer (beta)\nOnce you have access to a Kubernetes cluster, you can use the installer to setup magasin in that cluster.\nThe major goal of this installer is to ease the setup of some tools that need to be installed on your computer, and then deploy magasin to the kubernetes cluster.\n\n\n\n\n\n\nWarning\n\n\n\nIt is highly recommended to take a look at the install script before running it as it will install several components on your system.\nYou should run curl-bashing (curl piped with bash/zsh) only on providers that you trust. If you’re not confortable with this approach, proceed with the manual installation.\n\n\n\nDeploying magasin using a Debian/Like GNU/Linux computer\ncurl -sSL https://unicef.github.io/magasin/install-magasin.sh | bash\nDebian like distributions are Ubuntu, Raspbian, Kali Linux, etc. They use apt-get as package manager.\nDeploying magasin using a Mac OS computer\ncurl -sSL https://unicef.github.io/magasin/install-magasin.sh | zsh\n\nIn both cases, you need a user that can run sudo.\nThe installer is in beta, in case it fails check the troubleshooting section and, if the problems persist, try the manual installation\n\nDeploying magasin on a Windows computer Please use the manual installation",
    "crumbs": [
      "Home",
      "Install",
      "Installation"
    ]
  },
  {
    "objectID": "install/index.html#whats-next",
    "href": "install/index.html#whats-next",
    "title": "Installation",
    "section": "2 What’s next?",
    "text": "2 What’s next?\nCongratulations, now you have a magasin instance running on your kubernetes cluster. This is just the beggining of an interesting journey. Now you can:\n\nTake a look at magasin tutorial and start using magasin\nLearn about magasin’s architecture. Understand the different components and the underlying technology.\nLearn more about advanced installation. The steps showed in this page were the standard ways of installing magasin, but you can customize much more your setup.",
    "crumbs": [
      "Home",
      "Install",
      "Installation"
    ]
  },
  {
    "objectID": "install/advanced.html",
    "href": "install/advanced.html",
    "title": "Advanced installation",
    "section": "",
    "text": "This page provides further information that allows you to better understand how magasin is installed and how you can customize the default installation\nMagasin includes an installer script that sets up all the necessary dependencies on your computer, enabling seamless setup within the Kubernetes cluster.",
    "crumbs": [
      "Home",
      "Install",
      "Advanced installation"
    ]
  },
  {
    "objectID": "install/advanced.html#understanding-the-installer-script",
    "href": "install/advanced.html#understanding-the-installer-script",
    "title": "Advanced installation",
    "section": "1 Understanding the installer script",
    "text": "1 Understanding the installer script\nTo install magasin, the most straight forward way is to use the installer. This install-magasin.sh script is used for both Mac OS and Debian like GNU/Linux and performs two tasks:\n\nThe first one is to install all the pre-requisites required on your computer in order to be able to deploy magasin to a Kubernetes cluster\nThe second one is to deploy magasin in the preselected kubernetes cluster.\n\nThese are the steps the script takes:\n\nCheck if all the dependencies are already installed (namely kubectl, helm and python) and if any is missing it will prompt you to install it. It uses the recommended setup describe on the package for the Debian GNU/Linux like system. or in MacOS)\nRun the helm command install the different components of magasin in the kubernetes cluster. Note that each component will be installed in a different namespace within the cluster.",
    "crumbs": [
      "Home",
      "Install",
      "Advanced installation"
    ]
  },
  {
    "objectID": "install/advanced.html#magasin-realms",
    "href": "install/advanced.html#magasin-realms",
    "title": "Advanced installation",
    "section": "2 Magasin realms",
    "text": "2 Magasin realms\nCompared with most of applications that are distributed using helm charts, instead of defining one unique chart that includes all the dependencies, magasin defines an independent chart for each component. This results in a loosely-coupled architecture which allows you to only setup and use the components that fit your organizational needs.\nWhen deploying a containerized application in Kubernetes using Helm charts, typically, a singular chart is defined. This chart can actually be composed of multiple sub-charts, which uses a hierarchical structure within the deployment. This setup often leads to a well-defined architecture, beneficial for many scenarios, as it simplifies management and deployment.\nGiven that magasin setup uses independent charts, there is a need to setup at least one connection between these independent charts. This is done through a consistent naming convention which defines the concept of magasin realms.\nA realm is a way of linking independent helm charts through namespaces that follow consistent naming convention: --.\nBy convention, the default realm only has the prefix magasin and no postfix. For example, the namespace for the component drill that belongs to the default realm is magasin-drill.\nThe realm name is the concatenation of the prefix and the postfix, both separated by an hyphen - (if the postfix is not empty). Examples:\n\n\n\n\n\n\n\n\n\nRealm\nRealm prefix\nRealm postfix\nResulting namespace for “drill”\n\n\n\n\nmagasin\nmagasin\n-\nmagasin-drill\n\n\nmagasin-dev\nmagasin\ndev\nmagasin-drill-dev\n\n\nmagasin-new-version-dev\nmagasin-new-version\ndev\nmagasin-new-version-drill-dev\n\n\n-dev\n-\ndev\ndrill-dev\n\n\n\nA realm prefix can have - as part of its names, but not the realm postfix. In the realm name, the last - specifies the start of the postfix.\nIf a realm name starts with - it only has a postfix.\nThe concept of realms allows us setup supporting tools that can make use of two independent helm charts.",
    "crumbs": [
      "Home",
      "Install",
      "Advanced installation"
    ]
  },
  {
    "objectID": "install/advanced.html#advanced-use-of-the-magasin-installer",
    "href": "install/advanced.html#advanced-use-of-the-magasin-installer",
    "title": "Advanced installation",
    "section": "3 Advanced use of the magasin-installer",
    "text": "3 Advanced use of the magasin-installer\nYou can check the options.\n ./install-magasin.sh -h\nUsage: install-magasin.sh [-y] [-c] [-r realm_prefix-realm_postfix (magasin)] [-f values_folder (./)] [-d] [-h]\n\nThis script checks dependencies and installs magasin components\nEach component is installed within its own namespace.\n\nOptions:\n  -y  Skip prompting questions during installation\n  -c  Only check if all pre-requisites are installed in the local machine.\n  -i  Only install all pre-requisites in the local machine. Does not install magasin in Kubernetes\n  -r  Realm prefix and suffix (default: magasin). Prefix and suffix are separated by '-'.\n        If more than one '-', the last one will be used as separator.\n        The realm 'magasin-new-dev' will set 'magasin-new' as prefix and 'dev' as suffix.\n  -f  Folder with custom values.yaml files (default: ./).\n        Files within the folder shall have the same name as the component. Example:\n        drill.yaml, dagster.yaml, superset.yaml, daskhub.yaml\n  -u  URL/path to the magasin's helm repository (default: https://unicef.github.io/magasin/)\n      \n  -d  Enable debug mode (displays all commands run).\n  -h  Display this help message and exit.\n \nExamples of usage:\n\nAllows you to check if all the required components are installed\n  install-magasin.sh -c \nInstall all pre-requisites required and manage magasin within the current machine but without installing magasin. Installs all the missing components highlighted by the -c option install-magasin.sh -i This may be useful if you already have an instance of magasin and you want to seamlessly setup all the tools to manage that instance.\nSetup the realm test. Will use test-&lt;component&gt; as namespace.\n  install-magasin.sh -r test",
    "crumbs": [
      "Home",
      "Install",
      "Advanced installation"
    ]
  },
  {
    "objectID": "install/advanced.html#customizing-the-setup-of-each-component",
    "href": "install/advanced.html#customizing-the-setup-of-each-component",
    "title": "Advanced installation",
    "section": "4 Customizing the setup of each component",
    "text": "4 Customizing the setup of each component\nHelm charts allow you to customize some parameters such as the number of replicas, image version to load, startup script parameters, authentication schemas, etc. In order to do that you can create what is called a values file.\nThe installer allows you use custom values file for each component. By default it searches in the current working directory (./) for files that have the name &lt;component&gt;.yaml. That is dagster.yaml, drill.yaml, superset.yaml, daskhub.yaml.\nFor example, by default the Apache Drill helm chart launches two replicas of its main server. You can change the number of replicas by changing the drill.count value. You can create the file drill.yaml with the following contents:\n# drill.yaml\ndrill:\n  count: 1\nNow, if you run the installer in the same folder where you stored the drill.yaml file:\ninstall-magasin.sh\nYou’ll see in the logs something like:\n...\n i Installing magasin/drill in the namespace magasin-drill.\n ✓ Custom values file for drill exists (./drill.yaml)\n i helm install drill magasin/drill -f ./drill.yaml --namespace magasin-drill --create-namespace \n...\nBelow you have the default values files and the corresponding custom file name:\n\nDagster default values. To overwrite them create dagster.yaml\nApache Drill default values. To overwrite them create drill.yaml\nDaskhub default values. To overwrite them create daskhub.yaml\nSuperset default values. To overwrite them create superset.yaml\nMinIO Operator default values. To overwrite them create operator.yaml\nMinIO Tenant default values. To overwrite them create tenant.yaml\n\n\n4.1 Setting up different values for different environments\nWhereas for a testing environment you may want to have some values, such as setting to 1 number of replicas of the drill server. For your production environment you may want to have 3 replicas. The -f &lt;folder-path&gt; option allows you to select the folder where you have the customized values of the specific environment.\nGiven the structure below:\n  /\n  |- dev\n  |  |- drill.yaml\n  |\n  |- prd\n     |- drill.yaml\n     |- dagster.yaml\n     |- superset.yaml\nNow, you can deploy magasin for development by running:\ninstall-magasin.sh -r magasin-dev -f ./dev\nAnd the production environment by running:\ninstall-magasin.sh -r magasin-prd -f ./prd\nThanks to the concept of realms, you can have both instances even in the same cluster.",
    "crumbs": [
      "Home",
      "Install",
      "Advanced installation"
    ]
  },
  {
    "objectID": "install/advanced.html#whats-next",
    "href": "install/advanced.html#whats-next",
    "title": "Advanced installation",
    "section": "5 What’s next?",
    "text": "5 What’s next?\nCongratulations, now you have a magasin instance running on your kubernetes cluster. This is just the beggining of an interesting journey. Now you can:\n\nTake a look at magasin tutorial and start using magasin\nLearn about magasin’s architecture. Understand the different components and the underlying technology.\nLearn more about advanced installation. The steps showed in this page were the standard ways of installing magasin, but you can customize much more your setup.",
    "crumbs": [
      "Home",
      "Install",
      "Advanced installation"
    ]
  },
  {
    "objectID": "install/troubleshooting.html",
    "href": "install/troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "Please check the questions below. If you cannot find the answer you looking for you can try asking on magasin’s community discussions forum.\n\n\nIn case you run kubectl cluster-info and it is not pointing to the kubernetes cluster you want to use for magasin, you can use the following commands:\n\nGet the list of different contexts available\nkubectl config get-contexts\nWhich may output something like:\nCURRENT   NAME                        CLUSTER                   AUTHINFO                                              NAMESPACE\n\n          docker-desktop              docker-desktop            docker-desktop                                        default\n*         magasin                     docker-desktop            docker-desktop                                        default\n          magasin-dev                 docker-desktop            docker-desktop                                        default\n          minikube                    minikube                  minikube                                              default\nUse the correct cluster (i.e. context). Currently, the context magasin is selected, but to use magasin-dev you can run this command:\nkubectl config use-context magasin-dev\n\n\n\n\nIf you see this error \nand then when you see the status of the pods:\nkubectl get pods --namespace magasin-superset\nNAME                               READY   STATUS                  RESTARTS   AGE\nsuperset-68865d55bc-rvxq5          0/1     Init:ImagePullBackOff   0          68m\nsuperset-init-db-c9vcr             0/1     Init:ImagePullBackOff   0          68m\nsuperset-postgresql-0              1/1     Running                 0          68m\nsuperset-redis-master-0            1/1     Running                 0          68m\nsuperset-worker-6c65786947-vcwvx   0/1     Init:ImagePullBackOff   0          68m\nThen, your operating system may be using an ARM64 architecture. There is an issue with superset that does not provide multi-architecture images. By default the x86 architecture (also known as amd64) is provided in the helm chart.\nTo fix this issue, create a file called superset.yaml and paste the following contents below\n\n\nsuperset.yaml\n\nimage:\n  repository: apache/superset \n  tag: dfc614bdc3c8daaf21eb8a0d1259901399af7dd8-lean310-linux-arm64-3.9-slim-bookworm\n  pullPolicy: IfNotPresent\n\ninitImage:\n  repository: apache/superset\n  # ARM64 (M1, M2...)\n  tag: dfc614bdc3c8daaf21eb8a0d1259901399af7dd8-dockerize-linux-arm64-3.9-slim-bookworm\n  pullPolicy: IfNotPresent\n\nThen, manually install superset by running the command:\nhelm upgrade superset magasin/superset --namespace magasin-superset --create-namespace -f superset.yaml -i \n\n\n\nTo check if all the helm charts were properly installed, run this command\nhelm list --all-namespaces                                          \nIt may output something like\nNAME        NAMESPACE           REVISION    UPDATED                                 STATUS      CHART               APP VERSION       \ndagster     magasin-dagster     1           2024-01-26 08:00:11.343903 +0300 EAT    deployed    dagster-1.6.0       1.6.0             \ndaskhub     magasin-daskhub     1           2024-01-26 07:56:59.908074 +0300 EAT    deployed    daskhub-2024.1.0    jh3.2.1-dg2023.9.0\ndrill       magasin-drill       1           2024-01-26 07:56:50.414646 +0300 EAT    deployed    drill-0.6.1         1.21.1-3.9.1      \noperator    magasin-operator    1           2024-01-26 08:05:29.432176 +0300 EAT    deployed    operator-5.0.11     v5.0.11           \nsuperset    magasin-superset    1           2024-01-26 08:52:53.146091 +0300 EAT    failed      superset-0.10.15    3.0.1             \ntenant      magasin-tenant      1           2024-01-26 08:05:35.925144 +0300 EAT    deployed    tenant-5.0.11       v5.0.11     \nIn our case superset which was installed in magasin-superset namespace failed.\nWe can gather more information by getting the list of pods (containers) of superset’s namespace\nkubectl get pods --namespace magasin-superset\nNAME                               READY   STATUS                  RESTARTS   AGE\nsuperset-68865d55bc-rvxq5          0/1     Init:ImagePullBackOff   0          68m\nsuperset-init-db-c9vcr             0/1     Init:ImagePullBackOff   0          68m\nsuperset-postgresql-0              1/1     Running                 0          68m\nsuperset-redis-master-0            1/1     Running                 0          68m\nsuperset-worker-6c65786947-vcwvx   0/1     Init:ImagePullBackOff   0          68m\nLastly, we can get details of one of the pods that did not successfully launch using:\nkubectl describe pod superset-68865d55bc-rvxq5 --namespace magasin-superset\nEvents:\n  Type    Reason   Age                    From     Message\n  ----    ------   ----                   ----     -------\n  Normal  BackOff  2m38s (x276 over 66m)  kubelet  Back-off pulling image \"apache/superset:dockerize\"\nIn this case it seems there is an error getting the image. One option is to try to delete the pod. Kubernetes will create a new one\nkubectl delete pod superset-68865d55bc-rvxq5 --namespace magasin-superset\nThe new one will have a different name. So we need to run again kubectl get pods --namespace magasin-superset.\nIf this does not work, an alternative is to try to reinstall the helm chart manually. First uninstall the current chart as below:\nhelm uninstall superset --namespace magasin-superset\nrelease \"superset\" uninstalled\nThen install manually the magasin component. We will use the same command that was used by the installer (which you can see in the screenshot with the error)\nhelm install superset magasin/superset --namespace magasin-superset \nIf this does not work, you may try to find similar issues within the magasin discussion forum and/or file an issue.\n\n\n\nAfter a few minutes the installation of all the containers (pods) in the magasin-* namespaces should be in Complete or Running status. If there is any issue these may have another status.\nYou can check the status by running\nkubectl get pods --all-namespaces\nLet’s see an example:\nNAMESPACE          NAME                                                              READY   STATUS      RESTARTS        AGE\n...\n...\nmagasin-drill      drillbit-0                                                        0/1     Running     15 (102s ago)   102m\nmagasin-drill      drillbit-1                                                        0/1     Pending     0               102m\nmagasin-drill      zk-0                                                              1/1     Running     0               102m\nIn the case above we see that the drillbit-* are having issues. One is Running but had many restarts and the other is in Pending status. To inspect what’s going on you can run\nkubectl describe pod drillbit-1 --namespace magasin-drill\nName:             drillbit-0\nNamespace:        magasin-drill\nPriority:         0\nService Account:  drill-sa\nNode:             docker-desktop/192.168.65.3\nStart Time:       Fri, 12 Jan 2024 15:50:47 +0300\nLabels:           app=drill-app\n                  apps.kubernetes.io/pod-index=0\n                  controller-revision-hash=drillbit-b668897d5\n                  statefulset.kubernetes.io/pod-name=drillbit-0\nAnnotations:      &lt;none&gt;\nStatus:           Running\nIP:               10.1.0.106\nIPs:\n  IP:           10.1.0.106\nControlled By:  StatefulSet/drillbit\n\n...\n...\n\nEvents:\n  Type     Reason     Age                  From     Message\n  ----     ------     ----                 ----     -------\n  Normal   Pulled     41m (x3 over 54m)    kubelet  (combined from similar events): Successfully pulled image \"merlos/drill:1.21.1-deb\" in 28.915s (28.915s including waiting)\n  Warning  Unhealthy  26m (x135 over 99m)  kubelet  Readiness probe failed: Running drill-env.sh...\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nUser=drill\n/opt/drill/drillbit.pid file is present but drillbit is not running.\n[WARN] Only DRILLBIT_MAX_PROC_MEM is defined. Auto-configuring for Heap & Direct memory\n[INFO] Attempting to start up Drill with the following settings\n  DRILL_HEAP=1G\n  DRILL_MAX_DIRECT_MEMORY=3G\n  DRILLBIT_CODE_CACHE_SIZE=768m\n[WARN] Total Memory Allocation for Drillbit (4GB) exceeds available free memory (1GB)\n[WARN] Drillbit will start up, but can potentially crash due to oversubscribing of system memory.\n  Warning  Unhealthy  70s (x131 over 99m)  kubelet  Liveness probe failed: Running drill-env.sh...\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nUser=drill\n/opt/drill/drillbit.pid file is present but drillbit is not running.\n[WARN] Only DRILLBIT_MAX_PROC_MEM is defined. Auto-configuring for Heap & Direct memory\n[INFO] Attempting to start up Drill with the following settings\n  DRILL_HEAP=1G\n  DRILL_MAX_DIRECT_MEMORY=3G\n  DRILLBIT_CODE_CACHE_SIZE=768m\n[WARN] Total Memory Allocation for Drillbit (4GB) exceeds available free memory (1GB)\n[WARN] Drillbit will start up, but can potentially crash due to oversubscribing of system memory.\nAs we can see, there is an issue resulting from lack of sufficient memory. In this case we need to run the pod in a cluster with nodes having more RAM.",
    "crumbs": [
      "Home",
      "Install",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "install/troubleshooting.html#how-can-i-set-the-correct-kubernetes-cluster-in-kubectl",
    "href": "install/troubleshooting.html#how-can-i-set-the-correct-kubernetes-cluster-in-kubectl",
    "title": "Troubleshooting",
    "section": "",
    "text": "In case you run kubectl cluster-info and it is not pointing to the kubernetes cluster you want to use for magasin, you can use the following commands:\n\nGet the list of different contexts available\nkubectl config get-contexts\nWhich may output something like:\nCURRENT   NAME                        CLUSTER                   AUTHINFO                                              NAMESPACE\n\n          docker-desktop              docker-desktop            docker-desktop                                        default\n*         magasin                     docker-desktop            docker-desktop                                        default\n          magasin-dev                 docker-desktop            docker-desktop                                        default\n          minikube                    minikube                  minikube                                              default\nUse the correct cluster (i.e. context). Currently, the context magasin is selected, but to use magasin-dev you can run this command:\nkubectl config use-context magasin-dev",
    "crumbs": [
      "Home",
      "Install",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "install/troubleshooting.html#superset-error-initimagepullbackoff-during-installation-on-arm64-apple-m1-m2-architectures",
    "href": "install/troubleshooting.html#superset-error-initimagepullbackoff-during-installation-on-arm64-apple-m1-m2-architectures",
    "title": "Troubleshooting",
    "section": "",
    "text": "If you see this error \nand then when you see the status of the pods:\nkubectl get pods --namespace magasin-superset\nNAME                               READY   STATUS                  RESTARTS   AGE\nsuperset-68865d55bc-rvxq5          0/1     Init:ImagePullBackOff   0          68m\nsuperset-init-db-c9vcr             0/1     Init:ImagePullBackOff   0          68m\nsuperset-postgresql-0              1/1     Running                 0          68m\nsuperset-redis-master-0            1/1     Running                 0          68m\nsuperset-worker-6c65786947-vcwvx   0/1     Init:ImagePullBackOff   0          68m\nThen, your operating system may be using an ARM64 architecture. There is an issue with superset that does not provide multi-architecture images. By default the x86 architecture (also known as amd64) is provided in the helm chart.\nTo fix this issue, create a file called superset.yaml and paste the following contents below\n\n\nsuperset.yaml\n\nimage:\n  repository: apache/superset \n  tag: dfc614bdc3c8daaf21eb8a0d1259901399af7dd8-lean310-linux-arm64-3.9-slim-bookworm\n  pullPolicy: IfNotPresent\n\ninitImage:\n  repository: apache/superset\n  # ARM64 (M1, M2...)\n  tag: dfc614bdc3c8daaf21eb8a0d1259901399af7dd8-dockerize-linux-arm64-3.9-slim-bookworm\n  pullPolicy: IfNotPresent\n\nThen, manually install superset by running the command:\nhelm upgrade superset magasin/superset --namespace magasin-superset --create-namespace -f superset.yaml -i",
    "crumbs": [
      "Home",
      "Install",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "install/troubleshooting.html#explore-issues-within-the-installation",
    "href": "install/troubleshooting.html#explore-issues-within-the-installation",
    "title": "Troubleshooting",
    "section": "",
    "text": "To check if all the helm charts were properly installed, run this command\nhelm list --all-namespaces                                          \nIt may output something like\nNAME        NAMESPACE           REVISION    UPDATED                                 STATUS      CHART               APP VERSION       \ndagster     magasin-dagster     1           2024-01-26 08:00:11.343903 +0300 EAT    deployed    dagster-1.6.0       1.6.0             \ndaskhub     magasin-daskhub     1           2024-01-26 07:56:59.908074 +0300 EAT    deployed    daskhub-2024.1.0    jh3.2.1-dg2023.9.0\ndrill       magasin-drill       1           2024-01-26 07:56:50.414646 +0300 EAT    deployed    drill-0.6.1         1.21.1-3.9.1      \noperator    magasin-operator    1           2024-01-26 08:05:29.432176 +0300 EAT    deployed    operator-5.0.11     v5.0.11           \nsuperset    magasin-superset    1           2024-01-26 08:52:53.146091 +0300 EAT    failed      superset-0.10.15    3.0.1             \ntenant      magasin-tenant      1           2024-01-26 08:05:35.925144 +0300 EAT    deployed    tenant-5.0.11       v5.0.11     \nIn our case superset which was installed in magasin-superset namespace failed.\nWe can gather more information by getting the list of pods (containers) of superset’s namespace\nkubectl get pods --namespace magasin-superset\nNAME                               READY   STATUS                  RESTARTS   AGE\nsuperset-68865d55bc-rvxq5          0/1     Init:ImagePullBackOff   0          68m\nsuperset-init-db-c9vcr             0/1     Init:ImagePullBackOff   0          68m\nsuperset-postgresql-0              1/1     Running                 0          68m\nsuperset-redis-master-0            1/1     Running                 0          68m\nsuperset-worker-6c65786947-vcwvx   0/1     Init:ImagePullBackOff   0          68m\nLastly, we can get details of one of the pods that did not successfully launch using:\nkubectl describe pod superset-68865d55bc-rvxq5 --namespace magasin-superset\nEvents:\n  Type    Reason   Age                    From     Message\n  ----    ------   ----                   ----     -------\n  Normal  BackOff  2m38s (x276 over 66m)  kubelet  Back-off pulling image \"apache/superset:dockerize\"\nIn this case it seems there is an error getting the image. One option is to try to delete the pod. Kubernetes will create a new one\nkubectl delete pod superset-68865d55bc-rvxq5 --namespace magasin-superset\nThe new one will have a different name. So we need to run again kubectl get pods --namespace magasin-superset.\nIf this does not work, an alternative is to try to reinstall the helm chart manually. First uninstall the current chart as below:\nhelm uninstall superset --namespace magasin-superset\nrelease \"superset\" uninstalled\nThen install manually the magasin component. We will use the same command that was used by the installer (which you can see in the screenshot with the error)\nhelm install superset magasin/superset --namespace magasin-superset \nIf this does not work, you may try to find similar issues within the magasin discussion forum and/or file an issue.",
    "crumbs": [
      "Home",
      "Install",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "install/troubleshooting.html#after-installation-a-container-is-not-in-complete-or-running-status.",
    "href": "install/troubleshooting.html#after-installation-a-container-is-not-in-complete-or-running-status.",
    "title": "Troubleshooting",
    "section": "",
    "text": "After a few minutes the installation of all the containers (pods) in the magasin-* namespaces should be in Complete or Running status. If there is any issue these may have another status.\nYou can check the status by running\nkubectl get pods --all-namespaces\nLet’s see an example:\nNAMESPACE          NAME                                                              READY   STATUS      RESTARTS        AGE\n...\n...\nmagasin-drill      drillbit-0                                                        0/1     Running     15 (102s ago)   102m\nmagasin-drill      drillbit-1                                                        0/1     Pending     0               102m\nmagasin-drill      zk-0                                                              1/1     Running     0               102m\nIn the case above we see that the drillbit-* are having issues. One is Running but had many restarts and the other is in Pending status. To inspect what’s going on you can run\nkubectl describe pod drillbit-1 --namespace magasin-drill\nName:             drillbit-0\nNamespace:        magasin-drill\nPriority:         0\nService Account:  drill-sa\nNode:             docker-desktop/192.168.65.3\nStart Time:       Fri, 12 Jan 2024 15:50:47 +0300\nLabels:           app=drill-app\n                  apps.kubernetes.io/pod-index=0\n                  controller-revision-hash=drillbit-b668897d5\n                  statefulset.kubernetes.io/pod-name=drillbit-0\nAnnotations:      &lt;none&gt;\nStatus:           Running\nIP:               10.1.0.106\nIPs:\n  IP:           10.1.0.106\nControlled By:  StatefulSet/drillbit\n\n...\n...\n\nEvents:\n  Type     Reason     Age                  From     Message\n  ----     ------     ----                 ----     -------\n  Normal   Pulled     41m (x3 over 54m)    kubelet  (combined from similar events): Successfully pulled image \"merlos/drill:1.21.1-deb\" in 28.915s (28.915s including waiting)\n  Warning  Unhealthy  26m (x135 over 99m)  kubelet  Readiness probe failed: Running drill-env.sh...\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nUser=drill\n/opt/drill/drillbit.pid file is present but drillbit is not running.\n[WARN] Only DRILLBIT_MAX_PROC_MEM is defined. Auto-configuring for Heap & Direct memory\n[INFO] Attempting to start up Drill with the following settings\n  DRILL_HEAP=1G\n  DRILL_MAX_DIRECT_MEMORY=3G\n  DRILLBIT_CODE_CACHE_SIZE=768m\n[WARN] Total Memory Allocation for Drillbit (4GB) exceeds available free memory (1GB)\n[WARN] Drillbit will start up, but can potentially crash due to oversubscribing of system memory.\n  Warning  Unhealthy  70s (x131 over 99m)  kubelet  Liveness probe failed: Running drill-env.sh...\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nUser=drill\n/opt/drill/drillbit.pid file is present but drillbit is not running.\n[WARN] Only DRILLBIT_MAX_PROC_MEM is defined. Auto-configuring for Heap & Direct memory\n[INFO] Attempting to start up Drill with the following settings\n  DRILL_HEAP=1G\n  DRILL_MAX_DIRECT_MEMORY=3G\n  DRILLBIT_CODE_CACHE_SIZE=768m\n[WARN] Total Memory Allocation for Drillbit (4GB) exceeds available free memory (1GB)\n[WARN] Drillbit will start up, but can potentially crash due to oversubscribing of system memory.\nAs we can see, there is an issue resulting from lack of sufficient memory. In this case we need to run the pod in a cluster with nodes having more RAM.",
    "crumbs": [
      "Home",
      "Install",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "privacy.html",
    "href": "privacy.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "The scope of this privacy policy is related with the Magasin Documentation Site with the URL http://unicef.github.io/magasin (Site)\nThis privacy policy does not cover the GitHub repository (https://github.com/unicef/magasin) and other external pages related with magasin."
  },
  {
    "objectID": "privacy.html#scope",
    "href": "privacy.html#scope",
    "title": "Privacy Policy",
    "section": "",
    "text": "The scope of this privacy policy is related with the Magasin Documentation Site with the URL http://unicef.github.io/magasin (Site)\nThis privacy policy does not cover the GitHub repository (https://github.com/unicef/magasin) and other external pages related with magasin."
  },
  {
    "objectID": "privacy.html#purpose",
    "href": "privacy.html#purpose",
    "title": "Privacy Policy",
    "section": "2 Purpose",
    "text": "2 Purpose\nThe purpose of collecting the data is to create statistics about the number of visits, to improve the documentation and the product itself."
  },
  {
    "objectID": "privacy.html#personal-data-collected",
    "href": "privacy.html#personal-data-collected",
    "title": "Privacy Policy",
    "section": "3 Personal data collected",
    "text": "3 Personal data collected\nThe personal data that is collected is:\n\nIP address\nBrowser\nOperating system\nScreen size\nPages visited within the site\nTime spent in the site"
  },
  {
    "objectID": "privacy.html#cookies",
    "href": "privacy.html#cookies",
    "title": "Privacy Policy",
    "section": "4 Cookies",
    "text": "4 Cookies\nThe only cookie that the Site uses is for collecting the statistics."
  },
  {
    "objectID": "privacy.html#sharing-the-collected-data",
    "href": "privacy.html#sharing-the-collected-data",
    "title": "Privacy Policy",
    "section": "5 Sharing the collected data",
    "text": "5 Sharing the collected data\n\nThe personal data collected by UNICEF is kept in UNICEF’s servers and is not shared with any other third party.\nThe personal collected data is only accessible by internal magasin team which is composed by UNICEF staff members.\nUNICEF may share annonymized data for reporting purposes."
  },
  {
    "objectID": "privacy.html#third-party-hosting",
    "href": "privacy.html#third-party-hosting",
    "title": "Privacy Policy",
    "section": "6 Third party hosting",
    "text": "6 Third party hosting\nThe Site is hosted under GitHub Pages Service. Please visit https://docs.github.com/en/site-policy/privacy-policies/github-privacy-statement"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Magasin (pronounced /ma.ɡa.zɛ̃/) is an end-to-end collection of open-source tools that are stitched together to provide enterprise ready automated ingestion, transformation, and storage as well as the enabling infrastructure and tools to perform data analysis, visualization of datasets and ML / AI.\nYou can learn more about why we created magasin and its architecture."
  },
  {
    "objectID": "about.html#about-magasin",
    "href": "about.html#about-magasin",
    "title": "About",
    "section": "",
    "text": "Magasin (pronounced /ma.ɡa.zɛ̃/) is an end-to-end collection of open-source tools that are stitched together to provide enterprise ready automated ingestion, transformation, and storage as well as the enabling infrastructure and tools to perform data analysis, visualization of datasets and ML / AI.\nYou can learn more about why we created magasin and its architecture."
  },
  {
    "objectID": "about.html#brief-history",
    "href": "about.html#brief-history",
    "title": "About",
    "section": "2 Brief history",
    "text": "2 Brief history\nMagasin grew out of Office of Innovation’s and Information and Communication Technology Division (ICTD) work to establish a RapidPro Data Warehouse to enable aggregation of global metrics across vendor instances.\nAlso informed by experiences of the Magic Box applied data science initiative, a range of components were evaluated and trialed with country offices to arrive at the current components and architecture.\nIn 2021, ICTD engaged one of its long term agreement vendors to assist with evolving the proof-of-concept into a minimum viable product (MVP).\nIn 2023, UNICEF started the journey to detach magasin from its organizational and cloud infrastructure dependencies, and release it as an open-source platform with the goal of becoming a Digital Public Good.\nIn February 2nd,2024, UNICEF released magasin as an open source solution. In this release, it was included an easy to install process, as well as how to get started documentation."
  },
  {
    "objectID": "get-started/create-a-dashboard.html",
    "href": "get-started/create-a-dashboard.html",
    "title": "Step 3: Create a dashboard",
    "section": "",
    "text": "In the previous steps we did an exploratory analysis that gave us the opportunity to learn about the data and what we could extract from it. After that we learnt to automate the data ingestion, that is the process of gathering and processing the data though in a scheduled way.\nSo now, we have our ready to use dataset which is automatically updated. The last step on the process is to create a beautiful dashboard that we can share with the decission makers to help them to take the organization to the next level thanks to the use of data informed decissions.\nIn magasin’s open source architecture, Apache Superset is the business intelligence dashboarding tool that comes out of the box, but similarly to what we mentioned about MinIO, the loosely coupled architecture of magasin allows organizations to choose to leverage other tools that the organization may be already using such as PowerBI or Tableau.\nApache Superset can consume any SQL based database engine. However, we stored our data as .parquet files. To allow us to use file based datasets in the architecture we have Apache Drill, which basically allows us to consume these datasets using a SQL interface. Indeed, you can also query other formats such as CSV or JSON and Apache Drill will provide a SQL interface to query them.",
    "crumbs": [
      "Home",
      "Get started",
      "Step 3: Create a dashboard"
    ]
  },
  {
    "objectID": "get-started/create-a-dashboard.html#setup-apache-drill-to-connect-to-minio",
    "href": "get-started/create-a-dashboard.html#setup-apache-drill-to-connect-to-minio",
    "title": "Step 3: Create a dashboard",
    "section": "1 Setup Apache Drill to connect to MinIO",
    "text": "1 Setup Apache Drill to connect to MinIO\n\nLaunch Apache Drill User interface\nmag drill ui\n\n\n\nDrill user interface home\n\n\n\nThen in the “Storage” page within the “Disabled Storage Plugins” click on the Update button of S3.\n\n\n\nDrill Update the S3 storage plugin\n\n\nModify the beggining of the JSON so it looks like:\n{\n  \"type\": \"file\",\n  \"connection\": \"s3a://magasin\",\n  \"config\": {\n    \"fs.s3a.connection.ssl.enabled\": \"false\",\n    \"fs.s3a.path.style.access\": \"true\",\n    \"fs.s3a.endpoint\": \"myminio-hl.magasin-tenant.svc.cluster.local:9000\",\n    \"fs.s3a.access.key\": \"minio\",\n    \"fs.s3a.secret.key\": \"minio123\"\n  },\n  \"workspaces\": {\n    \"data\": {\n      \"location\": \"/data\",\n      \"writable\": false,\n      \"defaultInputFormat\": null,\n      \"allowAccessOutsideWorkspace\": false\n    },\n    \"root\": {\n      \"location\": \"/\",\n      \"writable\": false,\n      \"defaultInputFormat\": null,\n      \"allowAccessOutsideWorkspace\": false\n    }\n  },\n  \"formats\": { \n    # ...\n    # KEEP THE REST OF THE FILE SAME AS THE ORIGINAL. \n    # ...\n  }\n}\nPress Update button, and then the click on the Enable button.\nNow, click on Query at the top navigation bar corner and submit this test query:\nUSE `s3`.`data`;\n\n\n\nDrill query UI\n\n\nIf every thing goes right, you should see something like:\n\n\n\nSuccess query result",
    "crumbs": [
      "Home",
      "Get started",
      "Step 3: Create a dashboard"
    ]
  },
  {
    "objectID": "get-started/create-a-dashboard.html#setting-up-the-drill-superset-database-connection",
    "href": "get-started/create-a-dashboard.html#setting-up-the-drill-superset-database-connection",
    "title": "Step 3: Create a dashboard",
    "section": "2 Setting up the Drill-Superset database connection",
    "text": "2 Setting up the Drill-Superset database connection\n\nLauch the Apache Superset user interface\nmag superset ui\nAs credentials, enter admin / admin\n\n\n\nSuperset login page:\n\n\n.\nGo to top right corner and go to Settings &gt; Database connections\n .\nClick on the + Database &gt; Select Apache Drill as database\n .\nAdd the below SQLAlechemy URI and, then, click on Test the connection.\ndrill+sadrill://drill-service.magasin-drill.svc.cluster.local:8047/dfs?use_ssl=False\n\n\n\nTest connection\n\n\nThis is a connection that comes out of the box with Apache Drill. If this works, it means that superset has access to Apache Drill. Now, we will use the storage we created earlier, the s3 connection.\nIf the connection suceeds, replace the string with the following and click on Test the connection\ndrill+sadrill://drill-service.magasin-drill.svc.cluster.local:8047/s3?use_ssl=False",
    "crumbs": [
      "Home",
      "Get started",
      "Step 3: Create a dashboard"
    ]
  },
  {
    "objectID": "get-started/create-a-dashboard.html#creating-the-superset-dashboard",
    "href": "get-started/create-a-dashboard.html#creating-the-superset-dashboard",
    "title": "Step 3: Create a dashboard",
    "section": "3 Creating the Superset Dashboard",
    "text": "3 Creating the Superset Dashboard\nOk, so we are in the final sprint to achieve our goal, but before we start, let’s do a quick introduction on some basic concepts of superset.\n\nSuperset is a tool for creating dashboards. Dashboards are composed by Charts.\nA chart is a graph that displays a some data that comes from a dataset.\nA dataset is a subset of the data that can be found in a database such as a table or, in our case a parquet file.\nA database is just a collection of tables, or in our case, a folder of parquet files.\n\nIf you take a look at the top navigation bar of superset, you will see precisely those items: Dashboards, Charts and Datasets.\nNow that we have some basics, let’s create our dashboard:\n\nClick on Dashboards at the top navigation bar, and then click on the ** + Dashboard** button\n .\nClick on the + Create new chart\nIn the new screen, click on add dataset\n .\nUsing the drop-downs, fill the data as in the image below:\n\nDatabase: Apache Drill (the one we just created earlier)\nSchema: s3.data\nTable: deployment_countries.parquet\n\n .\nClick on Create dataset and create chart button\nIn the list of chart types, select the Bar chart, and then Create new chart\n .\nFill the data as in the image below:\n\nX-Axis: deploymentCountries: The main column in the X axis\nX-Axis sort by: COUNT_DISTINCT(name) - We’re goint to sort them by counting the DPGs deployed for each country\nUncheck the X-Axis sort ascending – So we see first on the left the country with more DPGs deployed.\nRow limit: 10 – this will limit the number fo countries displayed to 10 (top ten)\n\nClick on Update chart button.\nClick on Save at the top right corner\n .\nIn the dialog box, give it a name and click on Save & go to dashboard\n .\n\n\n\n\n\n\n\nCongratulations! You have added your first chart to your dataset.\n\n\n\nAs an exercise you can complete the dashboard and add some more charts so it looks like this image:\n\n\n\nSuperset Dashboard\n\n\nAlternatively, you can import it:\n\nDownload the file dpga-superset-dashboard.zip.\nIn Dashboard, import the dashboard:\n\n\n\nsteps to import the dashboard",
    "crumbs": [
      "Home",
      "Get started",
      "Step 3: Create a dashboard"
    ]
  },
  {
    "objectID": "get-started/create-a-dashboard.html#awesome",
    "href": "get-started/create-a-dashboard.html#awesome",
    "title": "Step 3: Create a dashboard",
    "section": "4 Awesome!",
    "text": "4 Awesome!\nExcellent, you have completed this getting started, in the course of it, you were able to explore a dataset, create a pipeline and present your insights in a nice dashboard.",
    "crumbs": [
      "Home",
      "Get started",
      "Step 3: Create a dashboard"
    ]
  },
  {
    "objectID": "get-started/create-a-dashboard.html#whats-next",
    "href": "get-started/create-a-dashboard.html#whats-next",
    "title": "Step 3: Create a dashboard",
    "section": "5 What’s next",
    "text": "5 What’s next\nMagasin is composed by several mature open source components, mastering and setting them up to fulfill your organization requirements may require some time and practice.\n\nEnd user guides. If you want to learn more about from an end user perspective on how to use some of the components of magasin.\nCustom deployment and setup. This documents has useful references if you need to learn more on how to deploy magasin within your organization.",
    "crumbs": [
      "Home",
      "Get started",
      "Step 3: Create a dashboard"
    ]
  },
  {
    "objectID": "get-started/automate-data-ingestion.html",
    "href": "get-started/automate-data-ingestion.html",
    "title": "Step 2: Automate data ingestion",
    "section": "",
    "text": "This is the second step of the magasin getting started tutorial. In this step we will see how to automate the data ingestion.\nIn the previous step, we delved into the DPG Alliance API data, generating graphs and uncovering insights along the way.",
    "crumbs": [
      "Home",
      "Get started",
      "Step 2: Automate data ingestion"
    ]
  },
  {
    "objectID": "get-started/automate-data-ingestion.html#background",
    "href": "get-started/automate-data-ingestion.html#background",
    "title": "Step 2: Automate data ingestion",
    "section": "1 Background",
    "text": "1 Background\nTypically, after identifying intriguing insights, it’s common as next step to periodically update the data to monitor the evolution of the data.\nAutomating this process is highly advantageous as it eliminates the need for repetitive tasks. In our scenario, the workflow involves fetching data from the DPG API, followed by cleaning and processing it for seamless integration with a business intelligence dashboard creation tool, Superset, that will be used in the next step.\nThe good news is that we have already completed the heavy lifting using the Jupyter Notebook. This is advantageous because the code we will be writing is essentially the same; we just need to make some tweaks to adapt it into a Dagster pipeline.\n\n1.1 Advantages of using Dagster\nDagster is what is known as a pipeline orchestrator, which essentially helps us manage the ingestion of data from multiple sources.\nThe benefits of using a framework like Dagster are manifold. It allows us to approach tasks in a structured manner, facilitating scalability and monitoring. Anyone who has dealt with gathering data from multiple sources will attest that it can quickly turn into a nightmare if not managed properly. Dagster provides an excellent starting point for tackling such challenges.\nWhat usually happends when you start automating gathering data, processing and mixing data from multiple data sources is that:\n\nIt begins simple but with time it becomes more complex. You add more and more data sources, more logic to clean the data, stablish more dependencies between the data sources, etc. If this is not done properly you end up with a mess. Dagster provides us a framework that helps to write our code in a more structured way, as well as tools to debug and monitor the ingestion of the data.\nIt eventually may break. Data sources may change with time and with that your pipelines break. For instance, DPGA may change the API unexpectedly, and with that our automation will fail too. Dagster allows you to monitor failures as well as to debug where it failed.\n\nAnother advantage of Dagster, it that it uses Python as programming language, so, as we said earlier, most of the work we did for the Jupyter Notebook can be reused.\n\n\n1.2 Storing data as parquet files\nIn the magasin architecture, as general approach, we stand to store data assets as files. In particular, we recommend the use of Apache parquet file format.\nThe main reason to use files is:\n\nFirst, because it is an economic way to store data. Storage services in the cloud or in premises is relatively cheap.\nSecond, because it does provide more flexibility when changes on the underlying structure are introduced, at least compared with setting up a SQL database downstream.\nIn addition, it allows also to easily store more types of data such as documents or images. Lastly, in terms of governance and sharing the datasets, the problem is simplified to setting up file sharing permissions.\n\n\n\n1.3 Parquet format for structured data.\nParquet is a columnar storage file format that is commonly used in the Big Data ecosystem. It is designed to efficiently store and process large amounts of data.\nUsing Parquet to store processed datasets is beneficial because it optimizes storage, improves query performance, and enhances the overall efficiency of data processing. Its compatibility with popular Big Data processing frameworks and support for schema evolution make it a versatile choice for storing large-scale, evolving datasets in data lakes and data warehouses.\n\n\n1.4 MinIO, magasin’s storage layer (optional)\nFor keeping a consistent approach across the different cloud providers, magasin includes a component called MinIO. It gives an Amazon S3 compatible file (object) store.\nThanks to MinIO, regardless of what infrastructure you’re using, your pipelines will work the same, you do not need to adapt how you store the data if you move to another provider.\nWhereas to maintain a vendor-agnostic architecture we leverage MinIO. You have the flexibility to bypass this component if you find alternative methods more suitable for storing your ingested data. This decision may be based on better alignment with your existing processes or specific use case requirements.\nFor instance, instead of MinIO, you can directly utilize a vendor-specific object store like Azure Blobs, Amazon S3, or Google Cloud Storage. Alternatively, in contrast to the file-storage approach of the datasets you may choose to write your data directly into a database, such as DuckDB or PostgreSQL.\nFor the purposes of this tutorial we will be using MinIO. And the first step is to setup a bucket. A bucket is somewhat similar to a folder.\nSo, let’s start creating our pipeline.\n\n\n\n\n\n\nNote\n\n\n\nYou have available the source code of the complete project in magasin’s source code repository within the folderexamples/dpga-pipeline",
    "crumbs": [
      "Home",
      "Get started",
      "Step 2: Automate data ingestion"
    ]
  },
  {
    "objectID": "get-started/automate-data-ingestion.html#create-a-bucket-in-minio",
    "href": "get-started/automate-data-ingestion.html#create-a-bucket-in-minio",
    "title": "Step 2: Automate data ingestion",
    "section": "2 Create a bucket in MinIO",
    "text": "2 Create a bucket in MinIO\nThe first thing we need to do is to setup where we are going to store our data. Till now, we’ve been storing the data in our Jupyter lab space, but we need a place where we can securely save our datasets and access them through a standard API that is widely supported by other tools and libraries. MinIO is the component that will give us those capabilities.\nFirst, we are going to launch the MinIO API. This will forward a connection to our localhost:9000 from the MinIO API server in our cluster. This API allows us to interact with the MinIO storage accounts. In our case, we will use the MinIO client mc command line . mc is a tool similar to mag, but specific for MinIO. It is installed during the magasin installation.\nmag minio api\nKeep this running. In another terminal run this command:\n mc alias set myminio http://localhost:9000 minio minio123\n# mc alias set &lt;alias-name&gt; &lt;endpoint&gt; &lt;access-key&gt; &lt;secret-key&gt;\nAdded `myminio` successfully.\nNow, stop mag minio api command using Control + C.\nOnce we have configured the alias, we just need to create the bucket:\n mag minio add bucket --bucket-name magasin \nNote that the default alias that mag asumes will be myminio. If you used another alias you can use --alias option. For example:\nmag minio add bucket --bucket-name magasin --alias myalias\nAt this point, we have a bucket in MinIO which allows us to store files and access them through standard APIs.\n\n\n\n\n\n\nTip: mag shortcuts\n\n\n\nmag allows you to shorten the commands using the alias. For example:\nmag minio add bucket --bucket-name magasin --alias myalias`\ncan be written as\nmag m a b -b magasin -a myalias\nUsing the shorten version is a way of speeding up your interaction with the command, but it is less readable, so in this tutorial we will stick to the long version.\nTo get to know what are the available alias type mag --help or mag &lt;command&gt; --help you can see the shortcut versions in parenthesis.\nmag --help\nUsage: mag [OPTIONS] COMMAND [ARGS]...\n...\nCommands:\n  dagster (d)    Dagster commands\n  daskhub (dh)   Daskhub/Jupyterhub commands\n  drill (dr)     Apache Drill commands\n  minio (m)      MinIO commands\n  superset (ss)  Apache Superset commands\nHere you can see that daskhub shortcut is dh and minio is m",
    "crumbs": [
      "Home",
      "Get started",
      "Step 2: Automate data ingestion"
    ]
  },
  {
    "objectID": "get-started/automate-data-ingestion.html#create-a-dagster-pipeline",
    "href": "get-started/automate-data-ingestion.html#create-a-dagster-pipeline",
    "title": "Step 2: Automate data ingestion",
    "section": "3 Create a Dagster pipeline",
    "text": "3 Create a Dagster pipeline\nThe next step is to create a pipeline using Dagster. A pipeline is just a piece of code that moves data from place to another and that can introduce some changes before saving it in the destination place. In our case the pipeline will take the data from the DPGA API and store it in a MinIO bucket.\nThe first thing we need to do is to install Dagster.\npip install dagster==1.6.4 dagster-webserver==1.6.4\n\n\n\n\n\n\nNote\n\n\n\nDagster is a very agile product that is continuously evolving, this means that you have to be cognizant of the version you’re running.\nYou can check the version installed in your cluster by running helm list --all-namespaces and looking at the APP VERSION column.\nThen run pip install pip install dagster==&lt;version&gt;\n\n\n\n3.1 Add the pipeline code\nOnce Dagster is installed, we’re going to create a new project using the default structure prodivded by Dagster. This should be the default procedure for creating any new pipeline.\ndagster project scaffold --name dpga-pipeline\nCreating a Dagster project at /home/magasin/dpga-pipeline.\nCreating a Dagster code location at /home/magasin/dpga-pipeline.\nGenerated files for Dagster code location in /home/magasin/dpga-pipeline.\nGenerated files for Dagster project in /home/magasin/dpga-pipeline.\nSuccess! Created dpga-pipeline at /home/magasin/dpga-pipeline.\nBy scaffolding our project, Dagster creates a basic structure of a python package that could be installed using pip as any other package as well as some additional metadata files that will be used by Dagster to run the pipeline. You have some more info in the Dagster documentation.\nNow, lets add our code. Open the file dpga-pipeline/dpga_pipeline/assets.py\n\n\ndpga-pipeline/dpga_pipeline/assets.py\n\nimport requests\nimport pandas as pd\nfrom pandas import DataFrame\n\nfrom dagster import asset\n\n@asset\ndef raw_dpgs() -&gt; DataFrame:\n  \"\"\" DPGs data from the API\"\"\"\n  dpgs_json_dict = requests.get(\"https://api.digitalpublicgoods.net/dpgs\").json()\n  df = pd.DataFrame.from_dict(dpgs_json_dict)\n  return df\n\n@asset\ndef deployment_countries(raw_dpgs: DataFrame) -&gt; DataFrame:\n  df = raw_dpgs\n  df_loc = pd.merge(df, pd.json_normalize(df[\"locations\"]), left_index=True, right_index=True)\n  df_deployment_countries = df_loc.explode(\"deploymentCountries\")\n  df_deployment_countries[[\"name\",\"deploymentCountries\"]]\n\n  return df_deployment_countries\n\nAs you can see the code seems pretty similar to what we wrote in our exploratory analysis.\nThe in the code we have defined two @assets. An asset according to the Dagster definition is:\n\nAn asset is an object in persistent storage, such as a table, file, or persisted machine learning model. A Software-defined Asset is a Dagster object that couples an asset to the function and upstream assets used to produce its contents.\n\nIn our case, raw_dpgs, stores the dpgs as they come from the API as a DataFrame, and deployment_countries that extracts the one row per country in which the DPG has been deplayed.\nAnother thing that you can notice in the code is that in the definition of the deployment_countries asset, we are passing raw_dpgs: DataFrame. That will tell Dagster that deployment_countries depends on the raw_dpgs and it will be used as input.\nAs you noticed, we are using a couple of packages that need to be installed pandas and requests. To install them, in dpga-pipeline/setup.py we add them in the install_requires array.\n\n\ndagster-pipeline/setup.py\n\nsetup(\n  # ...\n  install_requires=[\n        \"dagster\",\n        \"dagster-cloud\",\n        \"pandas\",       # &lt;--- Add this line \n        \"requests\"      # &lt;---- Add this line too\n    ],\n  #...\n)\n\nOk, so now let’s test if this is working so far. To do that we will first install the pipeline package in editable mode (-e). This allows you to edit the package without needing to install it again.\npip install -e '.[dev]'\nThen, we will launch the Dagster user interface:\ndagster dev\nThis launches a local instance of dagster server in port 3000 on localhost. So just open http://localhost:3000. Note, instance of dagster is similar to what you are running on the cluster but directly on your computer. In this case you are not using the one installed in the cluster.\nYou should see something like:\n\n\n\nDagster user interface\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou have available the source code of the pipeline in magasin’s source code repository within the folder examples/dpga-pipeline/dpga-pipeline-store-local\n\n\n\n3.1.1 Save the assets in MinIO.\nTill now, we’ve been working on the development machine file system. The next step is to save the information we want to keep in MinIO.\nTo access the MinIO bucket we will use fsspec. This python library provides an standard interface regardless of the underlying filesystem. So, if you chose to use other file system to run this example, you can just change the environment variables and the address.\nMinIO provides an S3 compatible bucket file system, so we will use it. First we will add the dependencies fsspec and s3fs.\n\n\ndpga-pipeline/setup.py\n\nsetup(\n   #...\n    install_requires=[\n        \"dagster\",\n        \"dagster-cloud\",\n        \"pandas\",\n        \"requests\",\n        \"fsspec\",   # &lt;---- New dependency\n        \"s3fs\"      # &lt;---- New dependency\n    ],\n    #...\n)\n\nNow, we’re going to modify our assets to use the minio filesystem.\n\n\ndpga-pipeline/dpga_pipeline/assets.py\n\nimport fsspec\nimport requests\nimport pandas as pd\nfrom pandas import DataFrame\nfrom dagster import asset\n\n@asset\ndef raw_dpgs() -&gt; DataFrame:\n  \"\"\" DPGs data from the API\"\"\"\n  # Load from API\n  dpgs_json_dict = requests.get(\"https://api.digitalpublicgoods.net/dpgs\").json()  \n\n  # Convert to pandas dataframe\n  df = pd.DataFrame.from_dict(dpgs_json_dict)\n  return df\n\n@asset\ndef deployment_countries(raw_dpgs: DataFrame) -&gt; DataFrame:\n   \n  df = raw_dpgs\n  df_loc = pd.merge(df, pd.json_normalize(df[\"locations\"]), left_index=True, right_index=True)\n  df_deployment_countries = df_loc.explode(\"deploymentCountries\")\n  df_deployment_countries = df_deployment_countries[[\"id\", \"name\",\"deploymentCountries\"]]\n  \n  # Save to MinIO\n  fs= fsspec.filesystem('s3')\n  with fs.open('/magasin/data/deployment_countries.parquet','wb') as f:\n    df_deployment_countries.to_parquet(f)\n    \n  return df_deployment_countries\n\nThen, we will setup some environment variables that will setup the Minio S3 bucket credentials. Add the .env file in the root of your project (same folder as setup.py).\nFSSPEC_S3_ENDPOINT_URL='http://localhost:9000'\nFSSPEC_S3_KEY='minio'\nFSSPEC_S3_SECRET='minio123'\nAs you can see we are indicating in the .env file that the endpoint of our minio is in localhost port 9000. To enable this service we need to run the following command\nmag minio api\nAs earlier, while this command is running it will forward any connection in our localhost:9000 to the our MinIO instance in the Kubernetes cluster. You shoud keep running during this till you are instructed to do close it.\nIn another terminal, we need to reinstall the pipeline so the new dependencies are loaded, and, then, we can run Dagster:\n\npip install -e '.[dev]'\ndagster dev\nNote that after you launch dagster dev you should see something like:\ndagster - INFO - Loaded environment variables from .env file: \nFSSPEC_S3_ENDPOINT_URL,FSSPEC_S3_KEY,FSSPEC_S3_SECRET\nThis is because Dagster loads all the .env file automatically and exposes the variables to the code.\nOpen again the browser pointing to http://localhost:3000 and in the dagster UI and run Materialize all.\nThis time, all files should have been materialized in the magasin bucket.\nTo test if the files are there. In a terminal run:\nmc ls myminio/magasin/data\n\n\n\n3.2 Adding a job scheduler\nUntil now, we have been materializing manually our assets. However, automating this task is indeed the ultimate goal of setting up a pipeline.\nIn Dagster, you have available schedulers which basically run your pipeline, or pieces of it, in a fixed interval. Dagster schedulers follow a cron style format.\n\n\ndpga-pipeline/dpga_pipeline/assets.py\n\n#__init__.py\nfrom dagster import Definitions, load_assets_from_modules, define_asset_job, ScheduleDefinition\nfrom . import assets\n\nall_assets = load_assets_from_modules([assets])\n\n# Create an asset job that materializes all assets of the pipeline\nall_assets_job = define_asset_job(name=\"all_assets_job\",\n                                  selection=all_assets,\n                                  description=\"Gets all the DPG assets\")\n# Create a scheduler\nmain_schedule = ScheduleDefinition(job=all_assets_job,\n                                   cron_schedule=\"* * * * *\"\n                                   )\n\ndefs = Definitions(\n    assets=all_assets,\n    jobs=[all_assets_job],\n    schedules=[main_schedule]\n)\n\nWhat we did in the code above is to:\n\nAdd a job. A job, is basically a selection of assets that will be materialized together in the same run.\nDefine a schedule. The schedule will launch the job at specified time intervals. In our case every minute (* * * * *).\n\n\n\n\n\n\n\nTip: Understanding cron jobs\n\n\n\nThe job cron format is used to specify the schedule for recurring tasks or jobs in Unix-like operating systems and cron job scheduling systems. It consists of five fields separated by spaces, representing different aspects of the schedule:\n&lt;minute&gt; &lt;hour&gt; &lt;day-of-month&gt; &lt;month&gt; &lt;day-of-week&gt;\n\nMinute (0-59): Specifies the minute of the hour when the job should run. Valid values range from 0 to 59.\nHour (0-23): Specifies the hour of the day when the job should run. Valid values range from 0 to 23, where 0 represents midnight and 23 represents 11 PM.\nDay of Month (1-31): Specifies the day of the month when the job should run. Valid values range from 1 to 31, depending on the month.\nMonth (1-12): Specifies the month of the year when the job should run. Valid values range from 1 to 12, where 1 represents January and 12 represents December.\nDay of Week (0-7): Specifies the day of the week when the job should run. Both 0 and 7 represent Sunday, while 1 represents Monday, and so on, up to 6 representing Saturday.\n\nEach field can contain a single value, a list of values separated by commas, a range of values specified with a hyphen, or an asterisk (*) to indicate all possible values. Additionally, you can use special characters such as slashes (/) for specifying intervals and question marks (?) for leaving a field unspecified (e.g., for day of month or day of week when the other field should match).\nHere you have some examples of cron intervals\n\n\n\n\n\n\n\nCron Expression\nDescription\n\n\n\n\n0 0 * * *\nRun a task every day at midnight (00:00).\n\n\n15 2 * * *\nRun a task at 2:15 AM every day.\n\n\n0 0 * * 1\nRun a task every Monday at midnight (00:00).\n\n\n0 12 * * 1-5\nRun a task every weekday (Monday to Friday) at 12 PM (noon).\n\n\n*/15 * * * *\nRun a task every 15 minutes.\n\n\n0 */2 * * *\nRun a task every 2 hours, starting from midnight.\n\n\n30 3 * * 6\nRun a task every Saturday at 3:30 AM.\n\n\n0 0 1 * *\nRun a task at midnight on the first day of every month.\n\n\n0 0 1 1 *\nRun a task at midnight on January 1st every year.\n\n\n\n\n\nIf you launch again dagster dev and you go to Overview -&gt; Jobs, you can enable the job.\n\n\n\nScheduled job\n\n\n\n\n3.3 Deploy the pipeline in the cluster\nUntil now we have been running dagster on our own computer by enabling the access to the MinIO installed in our kubernetes cluster through mag minio api. But we want our pipeline to run entirely within our Kubernetes cluster. To do that we will deploy a container (pod) in our cluster that Dagster will use to run our pipeline.\nWe will follow this steps:\n\nPrepare the Docker image. Our pipeline will reside in a container that will be called by Dagster to run the pipeline. So we need to create a Docker image that will hold all our code and is ready to be called by Dagster.\nAdd the environment variables as secrets. We need to provide to our image the environmental variables. In Kubernetes this is done through secrets. Secrets are a special type of resource for holding sensitive information that exists in Kubernetes.\nRe-Deploy Dagster. After we have prepared our image with the pipeline, we need to tell our Dagster instance to deploy it, and use it. The simplest way is to re-deploy magasin’s dagster helm chart.\n\n\n3.3.1 Prepare the Docker image\nEdit the setup.py file of your project and add a new dependency dagster-postgres:\n\n\ndpga-pipeline/setup.py\n\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=\"dpga_pipeline\",\n    packages=find_packages(exclude=[\"dpga_pipeline_tests\"]),\n    install_requires=[\n        \"dagster\",\n        \"dagster-cloud\",\n        \"dagster-postgres\", #&lt;------------- Add this line\n        \"pandas\",\n        \"requests\",\n        \"fsspec\",\n        \"s3fs\"\n    ],\n    extras_require={\"dev\": [\"dagster-webserver\", \"pytest\"]},\n)\n\nThis is because now Dagster is going to use now the PosgreSQL database that is used in the cluster for keeping the logs data. Earlier, when we were using the local Dagster setup.\n\nIn the same folder as the `setup.py` file of your dagster project create a new file called [`Dockerfile`](https://docs.docker.com/engine/reference/builder/) and add the following.\n\n```{.sh filename=\"dagster-pipeline/Dockerfile\"}\n# Select the base image \nFROM python:3.10-slim\n\n# Copy all our code into the container\nCOPY . /\n\n# Install the module within the container\n# This will install all the dependencies\nRUN pip install .\n\nNow we are going to build the image. To prevent issues while running it, we are going to build a multi-architecture image. Currently, there are two major architectures we have to deal with ARM64 (RaspberryPi’s and Apple M1/M2…) and AMD64 (regular Intel and AMD computers). By building a multi-architecture image it will run regardless of the architecture.\nIf you’re use to create Docker images, something that you may have noticed is that in our Dockerfile we did not define an ENTRYPOINT or launched command CMD, in our Dockerfile basically we just installed our pipeline code. Whereas in Docker it is common to end the Dockerfile with one of these two commands, in our case the command that launches dagster will be injected during the deployment of the image. We will set it up later.\n# first we create a builder. This just allows us to build for architectures different that our owns.\n# This only needs to be run once per computer.\ndocker buildx create --driver=docker-container --name=magasin-builder \n\n# In the command below replace &lt;registry&gt; by your registry.\n# If you are using docker hub, it is your user name (you need to login first.\n# In other registries such as Azure Container Registry (my-registry.azurecr.io)or Amazon ECR, please check the documentation of the provider.\n\ndocker buildx build --builder=magasin-builder --platform linux/amd64,linux/arm64 -t &lt;registry&gt;/dpga-pipeline:latest --push  .\nNow in our registry we have a new image dpga-pipeline with the tag latest. Note that this image will be publicly available.\nFor the rest of the tutorial we will use this image: merlos/dpga-pipeline:latest, you can replace it with yours.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to check what other architectures are supported run:\n docker buildx ls\nNAME/NODE          DRIVER/ENDPOINT  STATUS  BUILDKIT             PLATFORMS\nmagasin-builder    docker-container                              \n  magasin-builder0 desktop-linux    running v0.12.3              linux/arm64, linux/amd64, linux/amd64/v2, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/mips64le, linux/mips64, linux/arm/v7, linux/arm/v6\n\n\n\n\n3.3.2 Add the environment variables as secrets\nPreviously, we set some environment variables with our credentials to access MinIO. When deploying an image to Kubernetes, the typical way to set sensitive information is through Secrets. Secrets is a simple way for us to set variables that are somewhat sensitive. For production deployments you should follow these good practices for Kubernetes secrets.\nkubectl create secret generic dpga-pipeline-secret \\\n  --namespace magasin-dagster \\\n  --from-literal=FSSPEC_S3_ENDPOINT_URL=http://myminio-ml.magasin-tenant.svc.cluster.local \\\n  --from-literal=FSSPEC_S3_KEY='minio' \\\n  --from-literal=FSSPEC_S3_SECRET='minio123'\nThis command will create a secret called dpga-pipeline-secret in the namespace magasin-dagster. Remember that a namespace in Kubernetes is something that can be compared to a folder.\nNote that the FSSPEC_S3_ENDPOINT_URL is no longer localhost, but the URL of the minio server on the cluster. Internal names follow this pattern &lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local.\nTo check the secret was created you can run this command:\nkubectl get secrets --namespace magasin-dagster\nAnd check there is a line with dpga-pipeline-secret with 3 in the data column:\nNAME                            TYPE                 DATA   AGE\ndagster-postgresql              Opaque               1      3d22h\ndagster-postgresql-secret       Opaque               1      3d22h\ndpga-pipeline-secret            Opaque               3      3m16s\nsh.helm.release.v1.dagster.v1   helm.sh/release.v1   1      3d22h\nTo see the contents of each data point:\nkubectl get secret dpga-pipeline-secret -n magasin-dagster -o jsonpath='{.data.FSSPEC_S3_ENDPOINT_URL}' | base64 --decode\nNotice the | base64 --decode, this is because the screts are encoded in base64. For example minio is encoded as bWluaW8=.\nIf you need to update the secret, one simple way is to delete and then add it back. To delete run the command:\n# kubectl delete secret &lt;secretname&gt; --namespace &lt;namespace-name&gt;\nkubectl delete secret dpga-pipeline-secret --namespace magasin-dagster\n\n\n3.3.3 Re-Deploy Dagster\nThe last thing we have to do is to re-deploy Dagster so that it includes our new pipeline.\nCreate a new file called dagster-helm-values.yml with the following contents:\ndagster-user-deployments:\n  enabled: true\n  deployments:\n    - name: \"dpga-pipeline-k8s\"\n      image:\n        repository: \"merlos/dpga-pipeline\"\n        tag: latest\n        pullPolicy: Always\n      dagsterApiGrpcArgs:\n        - \"--package-name\"\n        - \"dpga_pipeline\"\n      port: 3030\n      envSecrets:\n        - name: dpga-pipeline-secret\n      includeConfigInLaunchedRuns:\n        enabled: true\nThis file can also hold ConfigMaps or labels. You have more details about the dagster user deployment options\nThis file telling to include in the deployment our pipeline image (merlos/dpga-pipeline) as well as the environment secret envSecret called dpga-pipeline-secret.\nAlso we have defined in the file dagsterApiGrpcArgs. This includes the arguments for dagster api grpc, which you can get by running dagster api grpc --help. As we said earlier, it is on the deployment where we set launch command for the image. This is the command. Dagster uses Remote Procedure Calls, which for the purposes of this tutorial you can understand as an regular API to communicate the main dagster daemon and our deployments. The daemon is the long-lasting process that keeps track of the sensor,shedules, etc. And this daemon communicates with theIn our case we tell the command that Dagster uses remote procedure calls between the dagster main process and our image.\nNow we have to update our kubernetes deployment to include this new pipeline (a.k.a. code location in Dagster terminology).\nGo to the folder where the dagster-helm-values.yaml is located, and then run:\nhelm upgrade dagster magasin/dagster --namespace magasin-dagster -f ./dagster-helm-values.yml\nThis will update the deployment of the dagster instance of magasin. You should see something like:\nRelease \"dagster\" has been upgraded. Happy Helming!\nNAME: dagster\nLAST DEPLOYED: Tue Feb 13 09:28:32 2024\nNAMESPACE: magasin-dagster\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\nNOTES:\nLaunched. You can access the Dagster webserver/UI by running the following commands:\n\nexport DAGSTER_WEBSERVER_POD_NAME=$(kubectl get pods --namespace magasin-dagster -l \"app.kubernetes.io/name=dagster,app.kubernetes.io/instance=dagster,component=dagster-webserver\" -o jsonpath=\"{.items[0].metadata.name}\")\necho \"Visit http://127.0.0.1:8080 to open the Dagster UI\"\nkubectl --namespace magasin-dagster port-forward $DAGSTER_WEBSERVER_POD_NAME 8080:80\nTo open the Dagster user interface of the instance running in our Kubernetes cluster we need to run\nmag dagster ui\nNow, this will open the dagster instance in your Kubernetes cluster.\n\n\n\n\n\n\nNote\n\n\n\nYou have available the source code of the pipeline, Dockerfile, dagster-helm-values.yml in magasin’s source code repository within the folder examples/dpga-pipeline/dpga-pipeline-store-minio\n\n\n\n\n\n3.4 Troubleshooting the deployment\nIn case you face any issue here you have some ways of trying to find out what’s going on. This and seeking some help on a search engine or large language model, typically helps:\n\n\n3.5 Commands to inspect status\nCheck if everything is running fine. You can check the status of the pods in the magasin-dagster namespace\nkubectl get pods --namespace magasin-dagster\nNAME                                                              READY   STATUS    RESTARTS        AGE\ndagster-daemon-7c6474cbfd-7rgtr                                   1/1     Running   0               3h41m\ndagster-dagster-user-deployments-dpga-pipeline-k8s-5kqtc          1/1     Running   0               64m\ndagster-dagster-webserver-76ff9c7689-zv89b                        1/1     Running   0               3h41m\ndagster-postgresql-0                                              1/1     Running   6 (5h53m ago)   4d2h\ndagster-run-745684fc-80c5-45e5-a238-ce5fdc0c0dbe-nzh8x            0/1     Error     0               124m\nHere you can see the run had an error.\nDescribe the dagster-run pod:\nkubectl describe pod dagster-run-745684fc-80c5-45e5-a238-ce5fdc0c0dbe-nzh8x -n magasin-dagster\nGet the logs of the run pod:\nkubectl logs dagster-run-745684fc-80c5-45e5-a238-ce5fdc0c0dbe-nzh8x -n magasin-dagster\nkubectl describe job dagster-run-745684fc-80c5-45e5-a238-ce5fdc0c0dbe-nzh8x -n \nmagasin-dagster\nInspect the logs of the deployed main pod:\nkubectl logs dagster-dagster-user-deployments-dpga-pipeline-k8s-5kqtc --namespace magasin-dagster\n2024-02-13 10:26:42 +0000 - dagster.code_server - INFO - Starting Dagster code server for package dpga_pipeline on port 3030 in process 1\n2024-02-13 10:26:42 +0000 - dagster.code_server - INFO - Started Dagster code server for package dpga_pipeline on port 3030 in process 1\nLastly, on the Dagster user interface (launched with mag dagster ui), in the Runs tab, within your failed run click on View run button.",
    "crumbs": [
      "Home",
      "Get started",
      "Step 2: Automate data ingestion"
    ]
  },
  {
    "objectID": "get-started/automate-data-ingestion.html#summary",
    "href": "get-started/automate-data-ingestion.html#summary",
    "title": "Step 2: Automate data ingestion",
    "section": "4 Summary",
    "text": "4 Summary\nWe have done a big deal of tasks. This is, probably, the most challenging step as it requires working with many different tools and technologies (MinIO, Dagster, Helm, Kubectl, python…). However, using this setup, it offers us a lot of flexibility and allows us to scale.\n\nCreated a pipeline that loads from an API and saves the processed data as a .parquet file, a format that is Big Data friendly.\nWe created a MinIO bucket where we can save the output data in a storage that is widely supported by many systems and libraries (S3 bucket).\nWe scheduled our pipeline to be run automatically from time to time.\nLastly, we deployed this new pipeline within our Kubernetes cluster.",
    "crumbs": [
      "Home",
      "Get started",
      "Step 2: Automate data ingestion"
    ]
  },
  {
    "objectID": "get-started/automate-data-ingestion.html#whats-next",
    "href": "get-started/automate-data-ingestion.html#whats-next",
    "title": "Step 2: Automate data ingestion",
    "section": "5 What’s next",
    "text": "5 What’s next\nGo to next step, create a dashboard in Superset\nDagster has a learning curve, and it requires some work in order to get used to it, but the documentation is fairly good.\nHere you have a few links to get started with dagster:\n\nGet started with dagster 2.. Dagster Essentials",
    "crumbs": [
      "Home",
      "Get started",
      "Step 2: Automate data ingestion"
    ]
  },
  {
    "objectID": "get-started/tutorial-overview.html",
    "href": "get-started/tutorial-overview.html",
    "title": "Tutorial overview",
    "section": "",
    "text": "In this tutorial, we’ll walk you through a straightforward example we’ve created that demonstrates a typical data processing workflow. We will show you how to launch each component, explain essential concepts, but we won’t dive too deep into all the nitty-gritty details of each component.\nWe will follow this process simple process that is similar to what you may do in the real world:\nflowchart LR\n  A(1.Manual analysis using a \\nJupyter Notebook) --&gt; B(2.Automate data ingestion with\\n Dagster)\n  B --&gt; C(3.Create a Dashboard with\\n Superset)\nAs a result you will have the following dashboard that collects data from an external datasource, and is automatically updated through an schedule:",
    "crumbs": [
      "Home",
      "Get started",
      "Tutorial overview"
    ]
  },
  {
    "objectID": "get-started/tutorial-overview.html#step-1-exploratory-data-analysis-in-a-jupyter-notebook",
    "href": "get-started/tutorial-overview.html#step-1-exploratory-data-analysis-in-a-jupyter-notebook",
    "title": "Tutorial overview",
    "section": "0.1 Step 1: Exploratory data analysis in a Jupyter Notebook",
    "text": "0.1 Step 1: Exploratory data analysis in a Jupyter Notebook\nGenerally, before you start automating any process of regularly using some data you want to do some initial research and analysis, you want to discover and understand how the data is shaped and what kind of insights can be extract from the data you have. For that, Jupyter Notebook is a great tool. It allows you to run small pieces of code (usually Python or R) interactively in a user friendly interface.\nMagasin comes with the JupyterHub, which allows different users to run jupyter notebooks on the cloud without the need of installing anything on their own system.\nIn this tutorial, we are going play around with one simple API, the Digital Public Goods Alliance (DPGA) API.\nDigital Public Goods (DPGs) are defined as open source software, open data, open AI models, open standards and open content that adhere to privacy and other applicable laws and best practices, do no harm, and help attain the SDGs. Several international agencies, including UNICEF and UNDP, are exploring DPGs as a possible approach to address the issue of digital inclusion, particularly for children in emerging economies.\nFrom the data that we fetch from the API, we are going to query the data to derive some insights on how many DPGs are available, in which countries they are being deployed, where are they developed, what types of licenses are more popular…\nJupyter Notebooks are excellent for an initial analysis, but there are many use cases in which you may need to repeat the same process periodically. In our example, we may want to track if new DPGs are announced or what are the trends in terms of deployments. This is when the next steps of the process are useful.\nGo to step 1",
    "crumbs": [
      "Home",
      "Get started",
      "Tutorial overview"
    ]
  },
  {
    "objectID": "get-started/tutorial-overview.html#step-2-automate-data-ingestion-with-dagster",
    "href": "get-started/tutorial-overview.html#step-2-automate-data-ingestion-with-dagster",
    "title": "Tutorial overview",
    "section": "0.2 Step 2: Automate data ingestion with Dagster",
    "text": "0.2 Step 2: Automate data ingestion with Dagster\nAutomating is great, it frees us the burden of needing to repeat work. In our example, we will automate the ingestion and transformation of the DPGA API data. The data as it is served by the API is not fully ready to be analyzed, we need to perform some transformations to make it ready to be visualized.\nWe did the heavy-lifting interactively with the first analysis and now we just need to convert it to a dagster pipeline.\nDagster is a pipeline orchestrator, which basically helps us to manage the ingestion of data from multiple data sources.\nThe advantages of using a framework like dagster is that it will allow us to do things in a structured way which will allow you to scale and monitor workflows more efficiently. If you ask anyone that has been gathering data from multiple data sources they will tell you that it can become a daunting task if you don’t do it systematically. Dagster provides you a good starting point.\nWhat usually happends when you start automating stuff is that:\n\nIt starts simple but with time, you add more and more data sources that have dependencies between them. If it is not done properly you end up with a mess. Dagster provides us a framework to prevent that.\nIt may eventually break. Data sources may change with time and subsequently your pipelines may break. For instance, DPGA may update their API and causing the downstream automation to fail. Dagster enhances the monitoring capabilities for data pipelines.\n\nThe cool thing about Dagster, is that it uses the Python programming language too, so most of the work we did in the Jupyter Notebook is reusable.\nIn the magasin architecture, usually data assets are stored in files. In particular, we recommend the use of Apache parquet file format for storing the datasets.\nParquet is a columnar storage file format that is commonly used in the Big Data ecosystem. It is designed to efficiently store and process large amounts of data.\nUsing Parquet to store datasets is beneficial because it optimizes storage, improves query performance, and enhances the overall efficiency of data processing. Its compatibility with popular big data processing frameworks and support for schema evolution make it a versatile choice for storing large-scale, evolving datasets in data lakes and data warehouses.\nIn this step, we will leverage another component of magasin to help us maintain a cloud agnostic architecture. It is called MinIO and gives us an Amazon S3 compatible file store.\nThanks to MinIO, regardless of what infrastructure you’re using, your pipelines will work the same and you do not need to adapt how you store the data. Using files, compared to using a traditional SQL schema, allows you to store not only structured tabular data, but also images or documents. It simplifies also the governance of the datasets as the problem of sharing is reduced to assigning file/folder permissions.\nTo maintain a vendor-agnostic architecture, we leverage MinIO. However, in alignment with our loosely coupled design, you have the flexibility to bypass this component if you find alternative methods more suitable for storing your ingested data. This decision may be based on better alignment with your existing processes or specific use case requirements.\nFor instance, instead of MinIO, you can directly utilize a vendor-specific object store like Azure Blobs, Amazon S3, or Google Cloud Storage. Alternatively, you may choose to write your data into a database, such as DuckDB or PostgreSQL.\nGo to Step 2",
    "crumbs": [
      "Home",
      "Get started",
      "Tutorial overview"
    ]
  },
  {
    "objectID": "get-started/tutorial-overview.html#step-3-create-a-dashboard-with-superset",
    "href": "get-started/tutorial-overview.html#step-3-create-a-dashboard-with-superset",
    "title": "Tutorial overview",
    "section": "0.3 Step 3: Create a dashboard with Superset",
    "text": "0.3 Step 3: Create a dashboard with Superset\nDagster allows you to gather the data and transform it into something that can be displayed, but it does not come with advanced visualization capabilities. For that we need a tool that has better charting capabilities, and allows us to interact with the data, that is a Business Intelligence (BI) tool.\nmagasin ships with Apache Superset, which allows you to author dashboard and reports from a wide array of beautiful visualizations.\nIn this last step, we will create a dashboard to showcase the data.\nEarlier, we already did the heavy-lifting in the first step using the notebook environment, so now we just need familiar with Superset’s intuitive interface for visualizing datasets and crafting interactive dashboards.",
    "crumbs": [
      "Home",
      "Get started",
      "Tutorial overview"
    ]
  },
  {
    "objectID": "contributing/vulnerability-disclosure.html",
    "href": "contributing/vulnerability-disclosure.html",
    "title": "Vulnerability disclosure policy",
    "section": "",
    "text": "Magasin team welcomes feedback from security researchers and the general public to help improve our security. If you believe you have discovered a vulnerability, privacy issue, exposed data, or other security issues in any of our assets, we want to hear from you. This policy outlines steps for reporting vulnerabilities to us, what we expect, what you can expect from us.",
    "crumbs": [
      "Home",
      "Contributing",
      "Vulnerability disclosure policy"
    ]
  },
  {
    "objectID": "contributing/vulnerability-disclosure.html#introduction",
    "href": "contributing/vulnerability-disclosure.html#introduction",
    "title": "Vulnerability disclosure policy",
    "section": "",
    "text": "Magasin team welcomes feedback from security researchers and the general public to help improve our security. If you believe you have discovered a vulnerability, privacy issue, exposed data, or other security issues in any of our assets, we want to hear from you. This policy outlines steps for reporting vulnerabilities to us, what we expect, what you can expect from us.",
    "crumbs": [
      "Home",
      "Contributing",
      "Vulnerability disclosure policy"
    ]
  },
  {
    "objectID": "contributing/vulnerability-disclosure.html#systems-in-scope",
    "href": "contributing/vulnerability-disclosure.html#systems-in-scope",
    "title": "Vulnerability disclosure policy",
    "section": "2 Systems in Scope",
    "text": "2 Systems in Scope\nThis policy applies to any digital assets owned, operated, or maintained within the magasin open source project.\n\n2.1 Supported versions\nMagasin is currently in beta and continuously evolving. Given this, we only provide full support to the latest version released.\n\n\n\nVersion\nSupported\n\n\n\n\nlatest\nYes\n\n\nprevious\nNo",
    "crumbs": [
      "Home",
      "Contributing",
      "Vulnerability disclosure policy"
    ]
  },
  {
    "objectID": "contributing/vulnerability-disclosure.html#out-of-scope",
    "href": "contributing/vulnerability-disclosure.html#out-of-scope",
    "title": "Vulnerability disclosure policy",
    "section": "3 Out of Scope",
    "text": "3 Out of Scope\n\nAssets or other equipment not owned by parties participating in this policy.\n\nNote that Magasin makes use of several open source components such as Apache Drill, Dagster, MiniIO, Apache Superset, Dask, Jupyterhub…, including their Helm charts. Vulnerabilities discovered or suspected in these systems should be reported to the approriate vendor or applicable authority.",
    "crumbs": [
      "Home",
      "Contributing",
      "Vulnerability disclosure policy"
    ]
  },
  {
    "objectID": "contributing/vulnerability-disclosure.html#our-commitments",
    "href": "contributing/vulnerability-disclosure.html#our-commitments",
    "title": "Vulnerability disclosure policy",
    "section": "4 Our Commitments",
    "text": "4 Our Commitments\nWhen working with us, according to this policy, you can expect us to:\n\nRespond to your report promptly, and work with you to understand and validate your report;\nStrive to keep you informed about the progress of a vulnerability as it is processed;\nWork to remediate discovered vulnerabilities in a timely manner, within our operational constraints; and\nExtend Safe Harbor for your vulnerability research that is related to this policy.",
    "crumbs": [
      "Home",
      "Contributing",
      "Vulnerability disclosure policy"
    ]
  },
  {
    "objectID": "contributing/vulnerability-disclosure.html#our-expectations",
    "href": "contributing/vulnerability-disclosure.html#our-expectations",
    "title": "Vulnerability disclosure policy",
    "section": "5 Our Expectations",
    "text": "5 Our Expectations\nAs you participate in our vulnerability disclosure program, in good faith, we ask that you:\n\nPlay by the rules, including following this policy and any other relevant agreements. If there is any inconsistency between this policy and any other applicable terms, the terms of this policy will prevail;\nReport any vulnerability you’ve discovered promptly;\nAvoid violating the privacy of others, disrupting our systems, destroying data, and/or harming user experience;\nUse only the Official Channels to discuss vulnerability information with us;\nProvide us a reasonable amount of time (at least 90 days from the initial report) to resolve the issue before you disclose it publicly;\nPerform testing only on in-scope systems, and respect systems and activities which are out-of-scope;\nIf a vulnerability provides unintended access to data: limit the amount of data you access to the minimum required for effectively demonstrating a Proof of Concept; and cease testing and submit a report immediately if you encounter any user data during testing, such as Personally Identifiable Information (PII), Personal Healthcare Information (PHI), credit card data, or proprietary information;\nYou should only interact with test accounts you own or with explicit permission from the account holder; and\nDo not engage in extortion.",
    "crumbs": [
      "Home",
      "Contributing",
      "Vulnerability disclosure policy"
    ]
  },
  {
    "objectID": "contributing/vulnerability-disclosure.html#official-channels",
    "href": "contributing/vulnerability-disclosure.html#official-channels",
    "title": "Vulnerability disclosure policy",
    "section": "6 Official Channels",
    "text": "6 Official Channels\nPlease report security issues via an email to the UNICEF’s Information and Tecnology Division’s Digital Centre of Excellence : d _ c _ o _ e _ @ _ u _ n _ i _ c _ e _ f _ . _ o _ r _ g (remove the underscores and spaces), providing all relevant information. The more details you provide, the easier it will be for us to triage and fix the issue.",
    "crumbs": [
      "Home",
      "Contributing",
      "Vulnerability disclosure policy"
    ]
  },
  {
    "objectID": "contributing/vulnerability-disclosure.html#safe-harbor",
    "href": "contributing/vulnerability-disclosure.html#safe-harbor",
    "title": "Vulnerability disclosure policy",
    "section": "7 Safe Harbor",
    "text": "7 Safe Harbor\nWhen conducting vulnerability research, according to this policy, we consider this research conducted under this policy to be:\n\nAuthorized concerning any applicable anti-hacking laws, and we will not initiate or support legal action against you for accidental, good-faith violations of this policy;\nAuthorized concerning any relevant anti-circumvention laws, and we will not bring a claim against you for circumvention of technology controls;\nExempt from restrictions in our Terms of Service (TOS) and/or Acceptable Usage Policy (AUP) that would interfere with conducting security research, and we waive those restrictions on a limited basis; and\nLawful, helpful to the overall security of the Internet, and conducted in good faith.\n\nYou are expected, as always, to comply with all applicable laws. If legal action is initiated by a third party against you and you have complied with this policy, we will take steps to make it known that your actions were conducted in compliance with this policy.\nIf at any time you have concerns or are uncertain whether your security research is consistent with this policy, please submit a report through one of our Official Channels before going any further.\n\nNote that the Safe Harbor applies only to legal claims under the control of the organization participating in this policy, and that the policy does not bind independent third parties.",
    "crumbs": [
      "Home",
      "Contributing",
      "Vulnerability disclosure policy"
    ]
  },
  {
    "objectID": "contributing/index.html",
    "href": "contributing/index.html",
    "title": "Contributing",
    "section": "",
    "text": "Magasin follows an open community approach towards accepting contributions.\nIn this section you have access to specific documentation useful for developers who would wish to dive into the internals of magasin as well as the approach we are following regarding the processes and the documentation.\nOur main repo is Github-hosted at the site github.com/unicef/magasin. It hosts the main components, setup and admin scripts plus documentation.\nWe are following an adapted mono-repo approach for the larger trunk of the platform’s codebase. Below are the sub-projects of the core repository:\n\nHelm chart repository. Magasin, similarly to a GNU/Linux distribution is basically a collection of independent open source projects that are setup to work together in a common environment.\n\nIn general, our approach is get a copy of the officially maintained helm chart. However, there may be projects that do not have an official helm chart. In such cases we develop ours.\n\nmagasin installer. This is a shell script that allows us to install all the requirements (such as helm, kubectl, pip, mc) and the helm charts using one single command.\nmag CLI. This stands for Magasin command line interface. It provides a consistent interface for some common tasks leveraging the common approach and conventions of deploying magasin. It is a python utility based on click, a library for creating CLIs in a compostable way.\nProject documentation: Documentation is a key pillar for the platform. Magasin has a diverse audience of users with varying needs. In addition, magasin website is based on Quarto, a publishing platform which allows us to write markdown and generate an static website. It allows us to focus on the content.\n\nIn addition to these, we have other libraries and tools that are in other repositories.",
    "crumbs": [
      "Home",
      "Contributing",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing/helm-repo-dev.html",
    "href": "contributing/helm-repo-dev.html",
    "title": "Helm repo development",
    "section": "",
    "text": "This content is specially focused on the common tasks of magasin contributors and maintainers of the helm charts.\nThe core idea behind magasin is the easy deployment of complex cloud-ready applications in a Kubernetes cluster to enable a robust fit for purpose end-to-end data platform.\nWe use helm charts for provisioning the complex cloud-ready applications. Helm is a package manager such as pip, apt, brew, npm or gem.\nHelm uses a packaging format called charts. A chart is a collection of files that describe a related set of Kubernetes resources. A single chart might be used to deploy something simple, like a pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.\nIn general, our approach is to get the official helm chart provided by the maintainer of the component and then tweak the configuration to make it work seamlessly with the other components of magasin.\nThere may be cases where an official helm chart is non-existent. In those situations we have developed ours, as in the case of Apache Drill\nThe helm charts can be found in the helm folder within the main magasin GitHub repository.\nWe have a shell script, /helm-scripts/update-helm-charts.sh that collects the helm charts from the different official repos an saves them in our main repo. The script itself includes its documentation.\nThe repository itself is hosted by GitHub pages on the main magasin repo. The process of releasing the repo (i.e. updating the index.yaml and creating the helm packages)is managed by a CI/CD pipeline implemented using a GitHub action",
    "crumbs": [
      "Home",
      "Contributing",
      "Helm repo development"
    ]
  },
  {
    "objectID": "contributing/helm-repo-dev.html#working-with-the-charts",
    "href": "contributing/helm-repo-dev.html#working-with-the-charts",
    "title": "Helm repo development",
    "section": "1 Working with the charts",
    "text": "1 Working with the charts\nBelow you have some recipies to help you set up things up while modifying the helm charts.\nBefore working with the charts you need to setup a Kubernetes cluster.\n\n1.1 How can I update magasin’s repo helm charts?\nGenerally, magasin helm charts are just a copy of the “official” repositories. So, what we do is we download a copy of these externally developed charts. Then we test for interoperability to verify the magasin tools are working before release of the package. Magasin in that regard is similar to a GNU/Linux distribution.\nCurrently, we have a script that updates the folder /helm that contains magasin’s charts. It gets them from from other helm repos and copies them to the helm/ folder. The script is in /helm-scripts/update-helm-charts.sh.\n\nClone the repo\ngit clone https://github.com/unicef/magasin\nUpdate the versions editing /helm-scripts/update-helm-charts.sh\nFor now, it has the version numbers of the helm charts hard-coded. So, you have to modify it whenever you want to update the charts. The script file is documented and should guide you on how to proceed.\nRun the script.\n\n\n\n1.2 How can I test changes on helm charts?\nIf you have updated or modified the helm charts in the /helm folder, you may want to test them before performing a release.\nThe easiest way is to run a helm repo in your local machine, which basically is an HTTP server that has the helm charts packaged as .tgz files and a metadata file called index.yaml. Magasin includes a script that allows you to do this.\nLet’s wee what are the steps:\n\nClone the repo\ngit clone https://github.com/unicef/magasin\nMake the changes tn the helm charts within the /helm folder\nServe the helm charts in a local repo.\nGo to the helm-scripts and run the local-helm-repo.sh script.\n cd helm-scripts\n ./local-helm-repo.should\nThis script packages the helm charts in the /helm folder and launches an HTTP server that points to the folder /_helm-repo/ in the root folder of the repo in the port 8000. You can check it is running by opening a browser and pointing to the URL http://localhost:8000/index.yaml\nInstall magasin.\nGo to the installer folder and run the magasin installer specifying the url of the local repository\n cd magasin/installer\n ./install-magasin.sh -u http://localhost:8000\n\n\n\n1.3 What if I only want to update or install one single component?\nIn the previous section you had to install all the charts of magasin basing on the assumption that you did not have an instance already running. In the event that you already have an instance running and you want to update just a single chart. To do that you can run the helm command:\nAssuming you are in the root folder of the repo and you want to upgrade Apache Drill (helm/drill)\nIf you are installing the chart for the first time:\nhelm install drill ./helm/drill/  --namespace magasin-drill --create-namespace\nNote that the namespace should match the realm you’re working in. We’re asssuming the stardard namespace\nIf you’re updating the helm chart\nhelm upgrade drill ../../helm/drill/  --namespace magasin-drill\nIn both cases you can also add custom values by adding using the option -f &lt;path-to-my-values-yaml-file&gt;. For example:\nhelm upgrade drill ../../helm/drill/  --namespace magasin-drill -f drill.yaml\nNote that using this method, you won’t need to run the local helm repo script.",
    "crumbs": [
      "Home",
      "Contributing",
      "Helm repo development"
    ]
  },
  {
    "objectID": "contributing/repository-workflows.html",
    "href": "contributing/repository-workflows.html",
    "title": "Repository workflows",
    "section": "",
    "text": "The purpose of this section is to outline the workflow and guidelines for contributing code to this repository.",
    "crumbs": [
      "Home",
      "Contributing",
      "Repository workflows"
    ]
  },
  {
    "objectID": "contributing/repository-workflows.html#purpose",
    "href": "contributing/repository-workflows.html#purpose",
    "title": "Repository workflows",
    "section": "",
    "text": "The purpose of this section is to outline the workflow and guidelines for contributing code to this repository.",
    "crumbs": [
      "Home",
      "Contributing",
      "Repository workflows"
    ]
  },
  {
    "objectID": "contributing/repository-workflows.html#principles",
    "href": "contributing/repository-workflows.html#principles",
    "title": "Repository workflows",
    "section": "0.2 Principles",
    "text": "0.2 Principles\nOur principles are a set of fundamental guidelines that are meant to inform and streamline collaboration efforts. All our standardization decisions are based on the following principles:\n\nEnhance the developer experience. Standards ultimate goal is to create a pleasant development experience.\nProvide management key information. Workflows should enable management to tackle inefficiencies, pain points and increase output.\nAim for simplicity. If there are different options, we should aim for the simplest one.\n\nIn order to achieve these principles, Github is the main tool we use to track and manage product development related tasks, from requirements gathering to production release.",
    "crumbs": [
      "Home",
      "Contributing",
      "Repository workflows"
    ]
  },
  {
    "objectID": "contributing/repository-workflows.html#authors",
    "href": "contributing/repository-workflows.html#authors",
    "title": "Repository workflows",
    "section": "0.3 Authors",
    "text": "0.3 Authors\nThis document was created by Juan Merlos (@merlos)",
    "crumbs": [
      "Home",
      "Contributing",
      "Repository workflows"
    ]
  },
  {
    "objectID": "contributing/repository-workflows.html#branch-strategy",
    "href": "contributing/repository-workflows.html#branch-strategy",
    "title": "Repository workflows",
    "section": "1.1 Branch strategy",
    "text": "1.1 Branch strategy\nBranching in each repository will follow a trunk-based model. This model involves one eternal “trunk” called main from which all other branches originate.\nAll proposed changes or development work will take place on separate branches and will be merged into main via pull requests upon mandatory review/approval.\nThe goal of this approach is to have a highly stable codebase that is releasable on demand at all times.\nThe additional branches will be created for each feature, release, or hotfix. Each of these branches will have specific purposes and follow strict rules as to how they should be used. At a high level, the branching rules are described as follows:\n\n\n\nBranch type\nBranch from\nMerge to\nNaming convention\nExample\n\n\n\n\nfeature\nmain\nmain\nfeature/&lt;desc&gt;\nfeature/my-feature\n\n\nfix\nmain\nmain\nfeature/&lt;desc&gt;\nfeature/my-feature\n\n\nrelease\nmain\n-\nv&lt;semver&gt;\nv1.0.0\n\n\nhotfix\nmain\nmain, release\nhotfix/&lt;desc&gt;\nhotfix/my-fix\n\n\n\nThe purpose and correct usage of each supporting branch is elaborated in the following sub-sections. The graphic below can be used as an example of a complete workflow involving all branch types.\n\n\n\nBranch strategy flow\n\n\n\n1.1.1 Feature branches\nFor each new feature, or for updates to a given feature, a separate branch will be created. Feature branches must originate from the main branch. Since no development takes place on the main branch, feature branches are expected to be the most common type of branch.\nHowever, these branches should be short-lived. They will exist only as long as the feature is under development and never longer than a sprint. Ideally, a feature branch should involve one developer over a few (1-3) days of work.\nProduct backlog items (issues) should be defined with this in mind — if work on a feature is taking too long, then the item needs to be scoped differently (see the section on Delivery).\nOnce a feature branch is merged into main via pull request, it should be deleted. Any future work, even if it is related, will require a new branch.\nWhenever a feature branch is created, where possible, it should be related to an existing issue. Whereas issues will be tagged with the classification (bug, enhancement, etc), the naming of the feature branches is more free.\n\n\n1.1.2 Release branches\nRelease branches will be created for each version of the source that is destined for production release. This enables all preparation work for an upcoming release to take place even while development work for future releases is committed to the main.\nThe creation of a release branch will always correspond to an upcoming deployment. Since the main branch has a high standard of stability, it should be rare that further development work is needed to refine a release for production. If such a need arises, it will follow the hotfix strategy defined below.\nRelease branches never merge back into the trunk. A release branch should be representative of a release at a given point in time. There is no mandatory end of life for a release branch. These branches can exist indefinitely, as marker in release history and for potential rollback should the need arise.\nRelease branches follow the semantic version naming convention (i.e 1.0.0, 1.1.0). When a release branch is created it is also tagged with the same name.\n\n\n1.1.3 Hotfix branches\nHotfixes are small changes that need to be released to production more quickly (e.g. due to the severity of the issue). As with release and feature branches, they should always originate from main.\nUpon completion of the work, a hotfix branch will be merged into main. If the hotfix is also required in a release branch to be brought to production immediately, the change can be cherry-picked upon being merged into main. If no current release branch exists, one should be created for this purpose.\nOnce the corresponding pull requests complete, the temporary hotfix branch should be removed.\nWhenever there is a hotfix merged into a release branch, the commit that includes the fix shall be tagged updating the PATCH number, for instance, changing from 1.0.0 to 1.0.1.\nThis approach allows to provide long term support to a particular version.",
    "crumbs": [
      "Home",
      "Contributing",
      "Repository workflows"
    ]
  },
  {
    "objectID": "contributing/repository-workflows.html#pull-requests-and-commits-pr",
    "href": "contributing/repository-workflows.html#pull-requests-and-commits-pr",
    "title": "Repository workflows",
    "section": "1.2 Pull requests and commits (PR)",
    "text": "1.2 Pull requests and commits (PR)\nThe merging of supporting branches (feature and hotfix) into the main branch will always require a pull request.\nPull requests should include a reference to the issue numbers either in the commit or in the description of the PR. in the commit comment, using the following convention (where 0000 is the issue number and &lt;comments&gt; is placeholder for any optional comments).\n&lt;comments&gt; #0000 &lt;comments&gt; \nUsing comments like this will ensure that the pull request is automatically linked to all related issues.\nWhen submitting a pull request ensure:\n\nWell-formatted code*: Ensure that the committed code adheres to established coding standards and is well-formatted. Consistent indentation, proper naming conventions, and clean formatting improve code readability and maintainability.\nNo debugging or temporary code: Avoid including debugging statements, temporary code, or commented-out blocks. These can clutter the codebase and make it harder for others to understand and maintain the code.\nAvoid unrelated changes: The PR should focus on a specific feature. Avoid including unrelated changes in a single commit. If you have multiple unrelated changes, consider creating separate PR for each change.\nNo Sensitive information: Ensure that commits do not include any sensitive information like passwords, secrets, API keys, or personal data. Such information should be securely managed outside of the code repository.\nConsistency: Maintain a consistent style and structure throughout the project. Consistency helps establish good coding practices and makes it easier for the team to understand and collaborate on the codebase.\nDocumentation update: Ensure that the code, scripts, documentation and README files are up to date.\n\nCommit messages shall help the reviewer to understand what changes have been introduced. These are properties that commits shall have:\n\nAtomicity: Commits should be atomic, meaning they should represent a single logical change. It’s best to keep each commit focused on a specific task within the PR. This allows for easier code review, debugging, and reverting if necessary.\nClarity and descriptive messages: Commit messages should be clear, descriptive, and concise. They should summarize the purpose and content of the commit. A good commit message helps others understand the changes at a glance and provides context for future reference.\nExample. Too generic not descriptive message ``` ❌ Avoid Updates Readme\n✅ Better Add CONTRIBUTING link to Readme ```\nLogical progression: Commits should follow a logical progression, building upon previous commits. Each commit should leave the codebase in a stable and working state, ensuring that others can pull changes without introducing errors.\n\n\n1.2.1 Approvals\nCompletion of pull requests on main is generally restricted to designated approvers. The job of the approvers is to diligently review all code being merged to main to ensure the utmost stability of that branch.\nReviewers shall ensure the quality of the code, style and documentation. They shall ensure that it follows the best practices of the commits and pull requests exposed in this section.\nTheir aim should be to maintain a release-ready main branch at all times.",
    "crumbs": [
      "Home",
      "Contributing",
      "Repository workflows"
    ]
  },
  {
    "objectID": "why-magasin.html",
    "href": "why-magasin.html",
    "title": "Why magasin?",
    "section": "",
    "text": "In today’s data-informed world, governments and organizations face a monumental challenge: fragmented data spread across siloed systems. Departments, divisions, and units gather data independently, leading to inefficiencies and risks:\n\nFragmentation in data tools and capacity. Organizations, particularly those lacking centralized structures, often grapple with data tool fragmentation. This fragmentation is characterized by the use of diverse technologies across different teams, which can impede resource mobility and lead to technology duplication.\nMoreover, capacity issues frequently arise as a result of siloed work. This lack of integration exacerbates resource allocation challenges, thereby limiting the organization’s overall potential. Addressing these issues is crucial for enhancing operational efficiency and unlocking the full potential of the organization.\nMyopic data analysis and lack of comprehensive insights. Siloed data prevents organizations from gaining a holistic understanding of their operations and stakeholders, leading to shortsighted decision-making.\n\nTo overcome these challenges and unlock the full potential of modern data analysis, machine learning, and artificial intelligence, organizations need a comprehensive set of tools.",
    "crumbs": [
      "Home",
      "Welcome",
      "Why magasin?"
    ]
  },
  {
    "objectID": "why-magasin.html#the-challenge",
    "href": "why-magasin.html#the-challenge",
    "title": "Why magasin?",
    "section": "",
    "text": "In today’s data-informed world, governments and organizations face a monumental challenge: fragmented data spread across siloed systems. Departments, divisions, and units gather data independently, leading to inefficiencies and risks:\n\nFragmentation in data tools and capacity. Organizations, particularly those lacking centralized structures, often grapple with data tool fragmentation. This fragmentation is characterized by the use of diverse technologies across different teams, which can impede resource mobility and lead to technology duplication.\nMoreover, capacity issues frequently arise as a result of siloed work. This lack of integration exacerbates resource allocation challenges, thereby limiting the organization’s overall potential. Addressing these issues is crucial for enhancing operational efficiency and unlocking the full potential of the organization.\nMyopic data analysis and lack of comprehensive insights. Siloed data prevents organizations from gaining a holistic understanding of their operations and stakeholders, leading to shortsighted decision-making.\n\nTo overcome these challenges and unlock the full potential of modern data analysis, machine learning, and artificial intelligence, organizations need a comprehensive set of tools.",
    "crumbs": [
      "Home",
      "Welcome",
      "Why magasin?"
    ]
  },
  {
    "objectID": "why-magasin.html#marketplace-gaps",
    "href": "why-magasin.html#marketplace-gaps",
    "title": "Why magasin?",
    "section": "Marketplace gaps",
    "text": "Marketplace gaps\nWhen we go to the global market we find several gaps.\n\nOverwhelming landscape. Entering the world of data can be daunting, with a myriad of products each requiring trade-offs. Just look at the picture below that displays which was the DATA & AI Landscape already in 2019\n\n\n\nBig Data Landascape 2019 by Matt Turck. Source\n\n\nData/AI Leaders are solving a specific problems set. Most data systems are optimized for massive scale and low-latency, crucial for time-sensitive tasks like targeted advertising. However, not all organizations face such time-pressured scenarios.\nSystems that require a high cost entry. These data systems are generally not designed for low-end hardware or low cost of entry, further complicating the landscape for organizations exploring data solutions without having a deep pocket.\nEnd-to-end solutions are proprietary Traditional end-to-end data platforms often come with proprietary restrictions, limiting flexibility and tying organizations to specific cloud vendors or industry niches.\n\nTherefore, find in the market that there is not an end-to-end, open source solution that has a low cost of entry, that sets a uniform but flexible approach for different teams and that can scale with the needs of the organizaitons.\nHence, it is observed in the market that there lacks a comprehensive, open-source solution. Such a solution would ideally have a low entry cost, establish a uniform yet adaptable approach for various teams, and possess the capability to scale in accordance with the evolving needs of organizations. This gap presents a significant opportunity for innovation and development in the field.",
    "crumbs": [
      "Home",
      "Welcome",
      "Why magasin?"
    ]
  },
  {
    "objectID": "why-magasin.html#whats-next",
    "href": "why-magasin.html#whats-next",
    "title": "Why magasin?",
    "section": "What’s next",
    "text": "What’s next\n\nGetting started with magasin. Learn how to install, and start using magasin with a simple example.\nMagasin architecture. Learn more about the different components that conform magasin.",
    "crumbs": [
      "Home",
      "Welcome",
      "Why magasin?"
    ]
  },
  {
    "objectID": "architecture.html",
    "href": "architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "Magasin is a scalable end-to-end data platform based on open-source components that is natively run in a Kubernetes cluster.\nBy end-to-end this describes a data processing pipeline including from how to ingest raw data from multiple data sources, transform the data, run analyses on the processed data, storage in a cloud or local filesystem to enabling visualisation.\nKubernetes is a container orchestration system designed to automate the deployment, scaling, and management of containerized applications. It is an integral part of services offered by major cloud providers. Kubernetes, being open source, can also be set up on-premises. For testing purposes, it is even possible to install it on a desktop computer. By using kubernetes, we ensure the scalability of the solution.\nMagasin uses Kubernetes in combination with Helm, a package manager for Kubernetes applications that eases the initial setup of the different components of magasin. Helm is the equivalent to apt, pip, npm, pacman, snap, conda. Using Helm, users specify the configuration of required Kubernetes resources to deploy magasin through a values file or command-line overrides.\nThe main components of magasin architecture are independent mature open source projects. They are loosely coupled and glued through set of conventions and tools.",
    "crumbs": [
      "Home",
      "Welcome",
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#magasin-is-a-loosely-coupled-architecture",
    "href": "architecture.html#magasin-is-a-loosely-coupled-architecture",
    "title": "Architecture",
    "section": "1 magasin is a loosely-coupled architecture",
    "text": "1 magasin is a loosely-coupled architecture\nMagasin identifies the sets of needs for setting up an end-to-end data platform that can scale, and provides the set of open source components and an standard underlying technologies (helm, kubernetes) that can be used to solve the needs.\nHowever, organizations may already have some of the elements in the architecture already in place. For example, an organization may already have a preferred data visualization platform such as Microsoft PowerBI or Tableau, and switching to another visualization tool may entail more costs than the licenses themselves. Magasin gets organizations covered on that. Each of the components of magasin is not strictly required for an implementation.\nThis is achieved using helm packaged system in a slightly different way. Generally, in one single helm package all the components are including, setting up an more opinionated way of deploying a particular application. In magasin, each component operates as an autonomous helm chart. This design choice enables the establishment of a loosely-coupled architecture among its components. It allows you to install each component independently. Therefore, rather than mandating a rigid structure for the entire architecture, magasin embraces a more open and adaptable approach, fostering flexibility in component selection and integration.",
    "crumbs": [
      "Home",
      "Welcome",
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#magasins-components",
    "href": "architecture.html#magasins-components",
    "title": "Architecture",
    "section": "2 Magasin’s components",
    "text": "2 Magasin’s components\nMagasin is built on top of a set of mature open source projects to conform an base architecture for deploying an end-to-end data platform.\n\n\n\nMagasin architecture\n\n\n\n2.1 Ingestion: Dagster\nThe Dagster framework is the primary tool for orchestration of data pipelines for ingestion, transformation, analysis, and machine learning. Each pipeline is isolated and encapsulated on its own container, so different tasks may utilize different versions of a library.\nIn addition to pipeline isolation, it provides some advantages:\n\nA user interface that provides visibility of pipelines’ tasks, scheduling, run status, debugging messages, materialized assets, resources, and modes…\nDagster pipelines are written in python, which is a very familiar language for data scientist and data engineer.\nA framework for creating pipelines that scale. Whereas early in the data ingestion processes, pipelines are simple and straightforward, with time, when mixing different sources, which requires more complex cleaning and the need of armonizing identifiiers, the pipelines become very prone to be built as spaguetty code. This lmiits the maintainability, discoverability of issues and efficiency of introducing new changes. With dagster, you get a way of building pipelines that will be more structured and easy to maintain.",
    "crumbs": [
      "Home",
      "Welcome",
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#store-a-file-based-approach",
    "href": "architecture.html#store-a-file-based-approach",
    "title": "Architecture",
    "section": "3 Store: A file based approach",
    "text": "3 Store: A file based approach\nIn the magasin architecture, as general approach, we stand to store data assets as files. In particular, we recommend the use of Apache parquet file format.\nThe main reason to use a file based approach is:\n\nFirst, because it is an economic way to store data. Storage services in the cloud or in premises is relatively cheap.\nSecond, because it does provide more flexibility when changes on the underlying structure are introduced, at least compared with setting up a SQL database downstream.\nIn addition, it allows also to easily store more types of data such as documents or images.\nLastly, in terms of governance and sharing the datasets, the problem is simplified to setting up file sharing permissions.\n\nTo support this file based approach, there are two components that are introduced in the architecture. The first one is MinIO, which provides magasin with a layer that introduces agnosticity against the cloud provider. The second one is Apache Drill, which provides a SQL query engine that eases the extraction of insights from the files.\n\n3.0.1 MinIO: A cloud agnostic approach\nMagasin can be installed in any cloud provider on in premises. However, each cloud provider has a different service to store data. In order to provide a consistent way of storing data we have included as part of the standard list of components MinIO, a high-performance object storage system designed for cloud-native and containerized applications has been included as part of the magasin.\nFounded in 2014, MinIO offers an S3-compatible API, enabling seamless integration with existing cloud storage ecosystems. It is known for its simplicity, scalability, and speed, making it a popular choice for organizations seeking efficient data storage solutions. MinIO’s architecture is optimized for modern data workloads, leveraging erasure coding and distributed techniques to ensure data resilience and high availability. With its lightweight footprint and easy deployment on standard hardware, MinIO empowers developers to build scalable storage infrastructures tailored to their specific needs, whether for on-premises, hybrid, or multi-cloud environments.\nWhereas MinIO comes out of the box to provide uniform and cloud agnostic storage layer, there may be organizations that prefer to use a cloud native storage such as Azure Blob, S3 Buckets or Google Cloud Storage.\n\n\n3.0.2 Query engine: Apache Drill\nThe last piece of the file based approach is Apache Drill. Apache Drill is an open-source, schema-free query engine that provides a SQL interface to a wide range of non-relational datastores, such as NoSQL databases and collections of files such as JSON, CSV, ESRI shapefiles, SPSS & SAS formats, Parquet, and others.\nWhile data marts for specific business functions or locations traditionally require hosting and maintenance of a relational database on a server or virtual machine, Apache Drill enables comparable functionality without need for running and hosting a database or maintaining schema changes from source systems over time.\n\n\n3.1 Visualization: Apache Superset\nApache Superset is an open-source business intelligence product with comprehensive charting, dashboarding, and querying capabilities.\nApache Superset consumes SQL data sources, so Apache Drill is a nice companion for Superset to work with magasin’s file based approach.\n\n\n3.2 Analysis: Notebook environment JupyterHub\nA Jupyter notebook is an open-source allows you to create and share documents that contain live code, equations, visualizations, and narrative text. It is a powerful tool that combines code execution, rich text, mathematics, plots, and rich media into a single document. They are widely used in data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more.\nThe advantages of using Jupyter Notebooks are numerous. They provide an interactive computing environment that promotes exploratory analysis, making them ideal for data science projects. Notebooks can be easily shared, promoting reproducible research and facilitating collaboration. They also allow for inline plotting, providing immediate visual feedback on the data being analyzed. Typical uses of Jupyter Notebooks include data analysis and visualization, machine learning, statistical modeling, and educational purposes. For instance, a data scientist might use a Jupyter Notebook to clean a dataset, perform exploratory data analysis, build a machine learning model, and then visualize the results, all within a single, cohesive document.\nThe multi-tenant JupyterHub component creates on-demand, isolated containers for authenticated users, each with persistent storage for their R and Python notebook workspace.\n\n\n3.3 Parallel computing: Dask Gateway\nA Dask cluster is a flexible tool for parallel computing in Python. It allows you to write python code that will be run in multiples machines taking advantage of all the compute resources of the kubernetes cluster in which magasin is installed.\nIt is composed of a central scheduler and multiple distributed workers. Dask works well at many scales, ranging from a single machine to clusters of many machines. It enables parallel processing and extends the size of convenient datasets from “fits in memory” to “fits on disk”1. However, it can also work across a cluster of multiple machines.\nDask cluster Dask Gateway allows easy utilization of a Dask cluster from notebook environments for distributed computation of massive datasets or parallelizable operations.",
    "crumbs": [
      "Home",
      "Welcome",
      "Architecture"
    ]
  },
  {
    "objectID": "architecture.html#mag-cli",
    "href": "architecture.html#mag-cli",
    "title": "Architecture",
    "section": "4 Mag-cli",
    "text": "4 Mag-cli\nMag-cli is the command line interface of magasin. Helps to manage the different modules of magasin and it makes easier to perform common administration tasks.",
    "crumbs": [
      "Home",
      "Welcome",
      "Architecture"
    ]
  },
  {
    "objectID": "end-user-guides.html",
    "href": "end-user-guides.html",
    "title": "End user guides",
    "section": "",
    "text": "Each of the open source components included in magasin has its on end user documentation.",
    "crumbs": [
      "Home",
      "End user guides"
    ]
  },
  {
    "objectID": "end-user-guides.html#dagster",
    "href": "end-user-guides.html#dagster",
    "title": "End user guides",
    "section": "1 Dagster",
    "text": "1 Dagster\n\nGetting started\nDagster University",
    "crumbs": [
      "Home",
      "End user guides"
    ]
  },
  {
    "objectID": "end-user-guides.html#jupyter-hub",
    "href": "end-user-guides.html#jupyter-hub",
    "title": "End user guides",
    "section": "2 Jupyter Hub",
    "text": "2 Jupyter Hub\n\nProject Jupyter Documentation",
    "crumbs": [
      "Home",
      "End user guides"
    ]
  },
  {
    "objectID": "end-user-guides.html#apache-superset",
    "href": "end-user-guides.html#apache-superset",
    "title": "End user guides",
    "section": "3 Apache Superset",
    "text": "3 Apache Superset\n\nCreating your first dashboard\nExploring data in Superset",
    "crumbs": [
      "Home",
      "End user guides"
    ]
  },
  {
    "objectID": "contributing/repositories.html",
    "href": "contributing/repositories.html",
    "title": "Source Code Repositories",
    "section": "",
    "text": "unicef/magasin\n\nPrimary respository that hosts the following: main components, setup and admin scripts, as well as documentation",
    "crumbs": [
      "Home",
      "Contributing",
      "Source Code Repositories"
    ]
  },
  {
    "objectID": "contributing/repositories.html#magasin-main-project",
    "href": "contributing/repositories.html#magasin-main-project",
    "title": "Source Code Repositories",
    "section": "",
    "text": "unicef/magasin\n\nPrimary respository that hosts the following: main components, setup and admin scripts, as well as documentation",
    "crumbs": [
      "Home",
      "Contributing",
      "Source Code Repositories"
    ]
  },
  {
    "objectID": "contributing/repositories.html#libraries-and-utilities",
    "href": "contributing/repositories.html#libraries-and-utilities",
    "title": "Source Code Repositories",
    "section": "2 Libraries and utilities",
    "text": "2 Libraries and utilities\n\nunicef/superset-dashboard-cloner Scripts for cloning Apache Superset dashboards to create copies that use different data sources.",
    "crumbs": [
      "Home",
      "Contributing",
      "Source Code Repositories"
    ]
  },
  {
    "objectID": "contributing/repositories.html#custom-helm-charts",
    "href": "contributing/repositories.html#custom-helm-charts",
    "title": "Source Code Repositories",
    "section": "3 Custom Helm charts",
    "text": "3 Custom Helm charts\n\nunicef/magasin-drill A helm chart that enables setting up Apache Drill in cluster mode together with Zookeeper in a Kubernetes cluster.",
    "crumbs": [
      "Home",
      "Contributing",
      "Source Code Repositories"
    ]
  },
  {
    "objectID": "contributing/installer-dev.html",
    "href": "contributing/installer-dev.html",
    "title": "Installer (Dev)",
    "section": "",
    "text": "The [install-magasin.sh](https://github.com/unicef/magasin/blob/main/installer/install-magasin.sh) is a shell script that installs all the dependencies required to setup up all the helm charts that compose magasin.\nYou can find more details on what it does in the advanced installation page\nIt has a companion that is the [uninstall-script.sh](https://github.com/unicef/magasin/blob/main/installer/uninstall-magasin.sh)\nThe installer and uninstaller source code can be found under the installer/ folder within the main magasin repository.",
    "crumbs": [
      "Home",
      "Contributing",
      "Installer (Dev)"
    ]
  },
  {
    "objectID": "contributing/installer-dev.html#installer-tests",
    "href": "contributing/installer-dev.html#installer-tests",
    "title": "Installer (Dev)",
    "section": "1 Installer tests",
    "text": "1 Installer tests\nThe installer includes various functionality and options; in order to verify these are working we have created a CI/CD pipeline using Github actions that runs several tests in different operating systems.\nTo view the source code visit install-tests.yaml",
    "crumbs": [
      "Home",
      "Contributing",
      "Installer (Dev)"
    ]
  },
  {
    "objectID": "contributing/documentation.html",
    "href": "contributing/documentation.html",
    "title": "Documenting magasin",
    "section": "",
    "text": "Magasin is based on a handful of underlying technologies such as docker, kubernetes, helm and comprises several components which are mature and complex products by themselves; not to mention that all these technologies and components generally have very comprehensive documentation.\nFor that reason, instead of reinventing the wheel by duplicating all the existing documentation - the goal of magasin’s documentation is to provide practical and task-oriented tutorials or step by step guidance in the context of magasin.\nThe documentation should cover the needs of the following users in mind:\n\nNon-technical End Users (business users) - These are users who may be tech-savvy and are able to describe or design an application’s intended functionality, but do not participate in writing the code that constitutes an application. They are mostly interested in the visualization part, how to use the business intelligence dashboards, and the potential use cases of magasin applied to their business.\nAs a subset of these users, we can consider the business analysts who may be evaluating the solution.\nTechnical End Users - These are users with a technical background such as data analysts, data engineers and data scientists. They work mostly on the building data ingestion pipelines, running analyses and writing reports, and creating predictions models among other tasks.\nSystem Administrators (operations) - They are the responsible of setting up and maintaining an instance of magasin. They are concerned about permissions, security, reliability, monitoring…\nDevelopers and contributors - They are technical people that need to understand how magasin works internally in order to enhance the product itself.",
    "crumbs": [
      "Home",
      "Contributing",
      "Documenting magasin"
    ]
  },
  {
    "objectID": "contributing/documentation.html#documentation-policy",
    "href": "contributing/documentation.html#documentation-policy",
    "title": "Documenting magasin",
    "section": "",
    "text": "Magasin is based on a handful of underlying technologies such as docker, kubernetes, helm and comprises several components which are mature and complex products by themselves; not to mention that all these technologies and components generally have very comprehensive documentation.\nFor that reason, instead of reinventing the wheel by duplicating all the existing documentation - the goal of magasin’s documentation is to provide practical and task-oriented tutorials or step by step guidance in the context of magasin.\nThe documentation should cover the needs of the following users in mind:\n\nNon-technical End Users (business users) - These are users who may be tech-savvy and are able to describe or design an application’s intended functionality, but do not participate in writing the code that constitutes an application. They are mostly interested in the visualization part, how to use the business intelligence dashboards, and the potential use cases of magasin applied to their business.\nAs a subset of these users, we can consider the business analysts who may be evaluating the solution.\nTechnical End Users - These are users with a technical background such as data analysts, data engineers and data scientists. They work mostly on the building data ingestion pipelines, running analyses and writing reports, and creating predictions models among other tasks.\nSystem Administrators (operations) - They are the responsible of setting up and maintaining an instance of magasin. They are concerned about permissions, security, reliability, monitoring…\nDevelopers and contributors - They are technical people that need to understand how magasin works internally in order to enhance the product itself.",
    "crumbs": [
      "Home",
      "Contributing",
      "Documenting magasin"
    ]
  },
  {
    "objectID": "contributing/documentation.html#documentation-of-our-own-utilities",
    "href": "contributing/documentation.html#documentation-of-our-own-utilities",
    "title": "Documenting magasin",
    "section": "0.2 Documentation of our own utilities",
    "text": "0.2 Documentation of our own utilities\nIn order to enhance the user experience within magasin, we are developing custom a set of utilities, scripts and applications; that serve as a glue between components or that act as accelerators of common operations (such as backing up, upgrading etc)\n\nIt is a essential to document the source code including examples of usage for other future developers and contributors benefit.\nDepending on the use case for a given tool, there must be sysadmin and/or user documentation that explains how to use these utilities.\nFor those utilities and scripts that automate common processes (for example backing up the database of a component), corresponding documentation on the manual process will be provided. This is because each magasin component may be setup in multiple ways hence by providing the manual steps the user will have more control in case the assumed setup by the utility is not compatible with their environment.",
    "crumbs": [
      "Home",
      "Contributing",
      "Documenting magasin"
    ]
  },
  {
    "objectID": "contributing/documentation.html#website-release",
    "href": "contributing/documentation.html#website-release",
    "title": "Documenting magasin",
    "section": "1.1 Website release",
    "text": "1.1 Website release\nThe website is published in the Github Page. The resulting site is in the gh-pages branch of the main repository. The URL http://unicef.github.com/magasin.\n\n1.1.1 ** NEVER USE quarto publish gh-pages **\nWhereas quarto command line has the option of releasing to github pages through the command line you shall NEVER use it.\nThis command wipes out all the gh-pages contents and then puts a fresh version of the site. This is because magasin’s gh-page contains some additional files such as the helm repo index (index.yaml) and the installer scripts (install-magasin.sh) which are not managed by quarto publication.\nThe right way to release the website is to use publish-web GitHub Action whose source code is in the file .github/workflows/publish-web.yml within the main repo.",
    "crumbs": [
      "Home",
      "Contributing",
      "Documenting magasin"
    ]
  },
  {
    "objectID": "deployment.html",
    "href": "deployment.html",
    "title": "Production deployment",
    "section": "",
    "text": "Whereas the default installation of magasin is very convenient for testing, it is not ready for production.\nThe major concerns you should have when deploying to production should be:\nHere you have some links to reference documentation from the original projects that may help you to setup a production release.",
    "crumbs": [
      "Home",
      "Deployment",
      "Deploying to production"
    ]
  },
  {
    "objectID": "deployment.html#daskhub",
    "href": "deployment.html#daskhub",
    "title": "Production deployment",
    "section": "1 Daskhub",
    "text": "1 Daskhub\n\nDaskHub helm char\nJupyterHub helm Chart\nDask Gateway Helm\nDask Gateway Hem Chart Reference",
    "crumbs": [
      "Home",
      "Deployment",
      "Deploying to production"
    ]
  },
  {
    "objectID": "deployment.html#dagster",
    "href": "deployment.html#dagster",
    "title": "Production deployment",
    "section": "2 Dagster",
    "text": "2 Dagster\n\nDeployment\nDeploying with helm",
    "crumbs": [
      "Home",
      "Deployment",
      "Deploying to production"
    ]
  },
  {
    "objectID": "deployment.html#minio",
    "href": "deployment.html#minio",
    "title": "Production deployment",
    "section": "3 MinIO",
    "text": "3 MinIO\nFrom MinIO, magasin includes two helm charts: the operator and tenant. The operator basically allows you to deploy instances of tenants (provides multi-tenancy support). In the magasin default installation we do not use the operator to deploy the tenant but rather a helm chart, and we only install the operator as it is a requirement for a tenant. For one single organization, it may be enough to manage a single tenant.\n\nMinio Tenant Helm Chart Reference\nMinio Operator Helm Chart Reference\nDeploy minio operator with Helm\nDeploy a tenant with Helm",
    "crumbs": [
      "Home",
      "Deployment",
      "Deploying to production"
    ]
  },
  {
    "objectID": "deployment.html#apache-drill",
    "href": "deployment.html#apache-drill",
    "title": "Production deployment",
    "section": "4 Apache Drill",
    "text": "4 Apache Drill\n\nHelm chart Documentation\nApache Drill configuration\nConnecting Data Sources",
    "crumbs": [
      "Home",
      "Deployment",
      "Deploying to production"
    ]
  },
  {
    "objectID": "deployment.html#apache-superset",
    "href": "deployment.html#apache-superset",
    "title": "Production deployment",
    "section": "5 Apache Superset",
    "text": "5 Apache Superset\n\nHelm Chart documentation\nConfiguring Superset",
    "crumbs": [
      "Home",
      "Deployment",
      "Deploying to production"
    ]
  },
  {
    "objectID": "get-started/exploratory-analysis.html",
    "href": "get-started/exploratory-analysis.html",
    "title": "Step 1: Exploratory analysis",
    "section": "",
    "text": "This is the first out of the three steps of the getting started magasin tutorial. Generally, when you are analyzing data, you may want to learn about the data itself. Jupyter notebooks provide a nice and fast user programming interface that allows you to explore the data.\nIn this example we will explore the Digital Public Goods API",
    "crumbs": [
      "Home",
      "Get started",
      "Step 1: Exploratory analysis"
    ]
  },
  {
    "objectID": "get-started/exploratory-analysis.html#basic-analysis",
    "href": "get-started/exploratory-analysis.html#basic-analysis",
    "title": "Step 1: Exploratory analysis",
    "section": "1.1 Basic analysis",
    "text": "1.1 Basic analysis\nIn this basic analysis we are going to:\n\nDownload the list of DPGs from the API.\nTransform the data to get the number of deployments per country.\nDisplay a chart with the top 20 ountries that have more deployments.\n\nYou can download the resulting file with the jupyer notebook.\nFirst, let’s see some buttons of your newly created notebook you have the following buttons that will be required:\n\n\n\nJupyterhub basic buttons\n\n\nOk. so now we can start coding. Copy this code in the first cell and run the cell.\n!pip install requests pandas seaborn\nThis will install some python packages. You can run command-line commands by prepending ‘!’ to the command.\nNow, add a new cell, copy the code below and run the cell\nimport requests\nimport pandas as pd\n\ndpgs_json_dict = requests.get(\"https://api.digitalpublicgoods.net/dpgs\").json()\ndf = pd.DataFrame.from_dict(dpgs_json_dict)\n\ndf.head()\nNow that we have the DPG data, let’s proceed to the analysis.\nAdd a new cell with the below contents:\n# Extract deploymentCountries and developmentCountries from the locations column.\ndf_loc = pd.merge(df, pd.json_normalize(df[\"locations\"]), left_index=True, right_index=True)\n\n# Now we have two new columns in the dataframe. \n# Let's see the contents\ndf_loc[[\"deploymentCountries\", \"developmentCountries\"]]\nThese two new columns contain arrays each with the list of countries in which the DPG has been deployed and countries where de DPG has been developed.\nIf we run the cell below, if you have [India, Panama] in the deploymentCountries row of the DPG A, then there will be two rows for that DGP.\ndf_deployment_countries = df_loc.explode(\"deploymentCountries\")\ndf_development_countries = df_loc.explode(\"deploymentCountries\")\n\n# Check the output:\ndf_deployment_countries[[\"name\",\"deploymentCountries\"]]\nFinally, lets present a graph with the number of deployments per country.\n# Let's draw something\n\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n#ignoring case\ndeployments_per_country = df_deployment_countries\n                                .groupby(df_deployment_countries['deploymentCountries'].str.lower()) # group by country\n                                .size() # size of the group\n                                .sort_values(ascending=False)  # sort descending\n                                .reset_index(name=\"numberOfDPGs\")[:20] # Only 20\n\n#Display deployment countries and numberOfDPGs\nsns.set(rc={\"figure.figsize\":(20, 10)}) #Set size of the image\n# Tell what are the axis\nax = sns.barplot(y=\"deploymentCountries\", x=\"numberOfDPGs\", data=deployments_per_country )\n_ = ax.bar_label(ax.containers[0])\n\n# Set the titles of the graph and the axis\nplt.title(\"Number of DPGs deployed per country\", size=30)\nplt.xlabel(\"Number of DPGs\", size=20)\nplt.ylabel(\"To 20 Country of development\", size=20)\nYou should see something like this:\n\n\n\nGraph displayed after running the cell above",
    "crumbs": [
      "Home",
      "Get started",
      "Step 1: Exploratory analysis"
    ]
  },
  {
    "objectID": "get-started/exploratory-analysis.html#additional-explorations-optional",
    "href": "get-started/exploratory-analysis.html#additional-explorations-optional",
    "title": "Step 1: Exploratory analysis",
    "section": "1.2 Additional explorations (optional)",
    "text": "1.2 Additional explorations (optional)\nIn addition to this basic analysis, you can optionally see some additional exploratory analysis and details in the following notebook.\nDownload dpg-explorations.ipynb example.\nOnce you download it, drag and drop the fil to your Jupyter notebook environment within the left column where the list of files appear.",
    "crumbs": [
      "Home",
      "Get started",
      "Step 1: Exploratory analysis"
    ]
  },
  {
    "objectID": "get-started/exploratory-analysis.html#summary",
    "href": "get-started/exploratory-analysis.html#summary",
    "title": "Step 1: Exploratory analysis",
    "section": "1.3 Summary",
    "text": "1.3 Summary\nIn this step we have explored one of the components of Magasin, Jupyterhub, which provides us with an interactive user interface that allows us to explore our data one step (cell) at a time.\nThis component is specially oriented for data scientists and/or data engineers that are exploring how to convert the data into something that can be used to get insights for the business.\nOnce you found that this information is useful, typically, the next step is to automate creating the insights in a regular pace.",
    "crumbs": [
      "Home",
      "Get started",
      "Step 1: Exploratory analysis"
    ]
  },
  {
    "objectID": "get-started/exploratory-analysis.html#whats-next",
    "href": "get-started/exploratory-analysis.html#whats-next",
    "title": "Step 1: Exploratory analysis",
    "section": "1.4 What’s next",
    "text": "1.4 What’s next\n\nAutomate the data ingestion\nProject Jupyter Documentation",
    "crumbs": [
      "Home",
      "Get started",
      "Step 1: Exploratory analysis"
    ]
  },
  {
    "objectID": "get-started/index.html",
    "href": "get-started/index.html",
    "title": "Get started",
    "section": "",
    "text": "Magasin is a scalable end-to-end data platform based on open-source components that is natively run in a Kubernetes cluster.\nMagasin offers a value-for-money end-to-end data solution implementing a loosely-coupled architecture for organizations that need to setup a framework to scale the ingestion, storage, analysis and visualization of datasets. In addition, It also includes the capability of parallel computing for analyzing large datasets or AI model training.\nIn this getting started you will install magasin on your local machine for testing purposes, then you will perform an end-to-end data processing task that involves: exploratory analysis of a data source, creating a pipeline to automate data ingestion and authoring a dashboard to present your findings.\nBefore you continue, you may want to learn more about why magasin and its technical architecture, otherwise, let’s start by knowing the pre-requisite for installing magasin.",
    "crumbs": [
      "Home",
      "Get started",
      "Get started"
    ]
  },
  {
    "objectID": "get-started/index.html#install-pre-requisite-a-kubernetes-cluster",
    "href": "get-started/index.html#install-pre-requisite-a-kubernetes-cluster",
    "title": "Get started",
    "section": "1 Install pre-requisite: a Kubernetes cluster",
    "text": "1 Install pre-requisite: a Kubernetes cluster\nPrior to installing magasin, you need to have a Kubernetes cluster. But don’t worry, you can setup one on your local machine very easily. In layman terms, Kubernetes is just a technology that enables managing cloud ready applications, such as magasin.\nIn this getting started tutorial, we are going to setup a Kubernetes cluster through Docker Desktop, an application that can be installed on most computers. However, if you already have a cluster you can go directly to the install magasin section.\nFirst, install Docker Desktop. It is available for:\n\nGNU/Linux\nMac OS X\nWindows\n\nOnce installed. Go to Settings / Kubernetes , and enable Kubernetes. It will automatically install everything required, including the command line utility kubectl.\n\n\n\nScreenshot of Docker Desktop Kubernetes Settings that allows to enable Kubernetes\n\n\nIn addition go to ** Settings / Resources ** and give it as much as CPU and Memory with a minimum of 14GB.\n\n\n\nScreenshot of Docker Desktop Resource Settings\n\n\nLastly, on a command line, create the new cluster and use it:\nkubectl config set-context magasin --namespace default --cluster docker-desktop --user=docker-desktop\nkubectl config use-context magasin\nTo ensure that the kubernetes cluster is the correct one check if the name corresponds to the\nkubectl get nodes\nNAME             STATUS   ROLES           AGE   VERSION\ndocker-desktop   Ready    control-plane   48m   v1.28.2\nkubectl get namespaces\nNAME              STATUS   AGE\ndefault           Active   49m\nkube-node-lease   Active   49m\nkube-public       Active   49m\nkube-system       Active   49m\nAlternatively, you can also install minikube or if you have a cluster in any cloud provider you can also use it. At the end, you just need your kubectl to be setup to use whatever kubernetes cluster you want to use.",
    "crumbs": [
      "Home",
      "Get started",
      "Get started"
    ]
  },
  {
    "objectID": "get-started/index.html#install-magasin",
    "href": "get-started/index.html#install-magasin",
    "title": "Get started",
    "section": "2 Install magasin",
    "text": "2 Install magasin\nMagasin includes an installer script that sets up all the necessary dependencies on your computer, enabling the seamless setup within the Kubernetes cluster.\n\n\n\n\n\n\nWarning\n\n\n\nIt is highly recommended to take a look at the installer script before running as it will install several components on your system.\nYou should run curl-bashing (curl piped with bash/zsh) only on providers that you trust. If you’re not confortable with this approach, proceed with the manual installation.\n\n\nFor GNU/Linux Debian like\ncurl -sSL https://unicef.github.io/magasin/install-magasin.sh | bash\nFor MacOS devices\n curl -sSL https://unicef.github.io/magasin/install-magasin.sh | zsh\nFor Windows check the documentation for manual installation\nFor other systems please check the documentation for manual installation\nNote that the installation may take some minutes depending on the Internet connection speed of the machines running the cluster (mainly because of the container images).",
    "crumbs": [
      "Home",
      "Get started",
      "Get started"
    ]
  },
  {
    "objectID": "get-started/index.html#verify-everything-is-working",
    "href": "get-started/index.html#verify-everything-is-working",
    "title": "Get started",
    "section": "3 Verify everything is working",
    "text": "3 Verify everything is working\nAfter running the setup you can confirm that all the pods in the magasin-* namespace are in status Running or Complete\nkubectl get pods --all-namespaces \nNAMESPACE          NAME                                                              READY   STATUS      RESTARTS        AGE\nkube-system        coredns-5dd5756b68-fj7bj                                          1/1     Running     0               30d\nkube-system        coredns-5dd5756b68-qbjf4                                          1/1     Running     0               30d\nkube-system        etcd-docker-desktop                                               1/1     Running     0               30d\nkube-system        kube-apiserver-docker-desktop                                     1/1     Running     1 (16d ago)     30d\nkube-system        kube-controller-manager-docker-desktop                            1/1     Running     0               30d\nkube-system        kube-proxy-n8wwq                                                  1/1     Running     0               30d\nkube-system        kube-scheduler-docker-desktop                                     1/1     Running     5               30d\nkube-system        storage-provisioner                                               1/1     Running     5 (16d ago)     30d\nkube-system        vpnkit-controller                                                 1/1     Running     0               30d\nmagasin-dagster    dagster-daemon-5cbb759cbd-gzczz                                   1/1     Running     0               31m\nmagasin-dagster    dagster-dagster-user-deployments-k8s-example-user-code-1-8qcjnt   1/1     Running     0               31m\nmagasin-dagster    dagster-dagster-webserver-755f9bc489-w9jdw                        1/1     Running     0               31m\nmagasin-dagster    dagster-postgresql-0                                              1/1     Running     0               31m\nmagasin-daskhub    api-daskhub-dask-gateway-6b7bf7ff6b-qqnjz                         1/1     Running     0               31m\nmagasin-daskhub    continuous-image-puller-jf6cd                                     1/1     Running     0               31m\nmagasin-daskhub    controller-daskhub-dask-gateway-7f4d8b9475-bfzg6                  1/1     Running     0               31m\nmagasin-daskhub    hub-6848dd9966-zxh7k                                              1/1     Running     0               31m\nmagasin-daskhub    proxy-797fc4d885-rrx4t                                            1/1     Running     0               31m\nmagasin-daskhub    traefik-daskhub-dask-gateway-6555db458-vp6xs                      1/1     Running     0               31m\nmagasin-daskhub    user-scheduler-5d8967fc5f-bfjt9                                   1/1     Running     0               31m\nmagasin-daskhub    user-scheduler-5d8967fc5f-tmn8r                                   1/1     Running     0               31m\nmagasin-drill      drillbit-0                                                        1/1     Running     0               33m\nmagasin-drill      drillbit-1                                                        1/1     Running     0               33m\nmagasin-drill      zk-0                                                              1/1     Running     0               33m\nmagasin-operator   console-654bf548c-5xf45                                           1/1     Running     0               30m\nmagasin-operator   minio-operator-7496fbc5d9-j82ml                                   1/1     Running     0               30m\nmagasin-operator   minio-operator-7496fbc5d9-znppq                                   1/1     Running     0               30m\nmagasin-superset   superset-7c88fcc74f-lrjwk                                         1/1     Running     0               31m\nmagasin-superset   superset-init-db-75rht                                            0/1     Completed   0               31m\nmagasin-superset   superset-postgresql-0                                             1/1     Running     0               31m\nmagasin-superset   superset-redis-master-0                                           1/1     Running     0               31m\nmagasin-superset   superset-worker-df94c5947-mw6k7                                   1/1     Running     0               31m\nIf you have any issue, check the troubleshooting section\n\n\n\n\n\n\nImportant\n\n\n\nThe default installation is fine for testing purposes, but for a production environment you should follow the production deployment guides",
    "crumbs": [
      "Home",
      "Get started",
      "Get started"
    ]
  },
  {
    "objectID": "get-started/index.html#next-steps",
    "href": "get-started/index.html#next-steps",
    "title": "Get started",
    "section": "4 Next steps",
    "text": "4 Next steps\nOk, now you have a fully running instance of magasin in your Kubernetes cluster, so what now:\n\nStart using magasin. We have created a tutorial that will take you through the typical steps for creating an end-to-end data processing pipeline and consequently for enabling a data-driven organization using magasin.\nAlso, you can learn more about the components and architecture of magasin. Learn more about the the different components that come out of the box with magasin.",
    "crumbs": [
      "Home",
      "Get started",
      "Get started"
    ]
  },
  {
    "objectID": "security.html",
    "href": "security.html",
    "title": "Security",
    "section": "",
    "text": "Ensuring robust security measures within a Kubernetes cluster hosting diverse open-source applications is paramount in safeguarding sensitive data and maintaining operational integrity.\nIf you want to disclose a potential vulnerability, please read our vulnerability disclosure policy.",
    "crumbs": [
      "Home",
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "security.html#reference-documentation",
    "href": "security.html#reference-documentation",
    "title": "Security",
    "section": "1 Reference documentation",
    "text": "1 Reference documentation\nEach component of magasin has its own level of security\n\nKubernetes Security\nJupterHub Security\nDask Gateway Security\nMinIO Security:\n\nChecklist\nIdentity Access Management\nServer Side Encryption\nData Encryption\nNetwork Encryption\n\nApache Drill Security\nApache Zookeeper\nApache Superset Security",
    "crumbs": [
      "Home",
      "Deployment",
      "Security"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "magasin is the cloud native open-source end-to-end data platform\n\n\nmagasin enables organizations to perform of automatic data ingestion, storage, analysis, ML/AI compute and visualization at scale\n\n\n\nGet started Why magasin"
  },
  {
    "objectID": "docs-home.html",
    "href": "docs-home.html",
    "title": "Magasin documentation",
    "section": "",
    "text": "Magasin is a cloud native open-source end-to-end data platform. Magasin enables organizations to perform automatic data ingestion, storage, analysis, ML/AI compute and visualization at scale.",
    "crumbs": [
      "Home",
      "Welcome",
      "Welcome"
    ]
  },
  {
    "objectID": "docs-home.html#learn-more-about-magasin",
    "href": "docs-home.html#learn-more-about-magasin",
    "title": "Magasin documentation",
    "section": "1 Learn more about magasin",
    "text": "1 Learn more about magasin\n\nWhy magasin?\nArchitecture",
    "crumbs": [
      "Home",
      "Welcome",
      "Welcome"
    ]
  },
  {
    "objectID": "docs-home.html#start-using-magasin",
    "href": "docs-home.html#start-using-magasin",
    "title": "Magasin documentation",
    "section": "2 Start using magasin",
    "text": "2 Start using magasin\n\nGet Started. Quick setup\nInstallation. Detailed explanation of the base installation.\nEnd user guides. References on how to use the components\nDeployment. Learn more about the configuration of the different components.",
    "crumbs": [
      "Home",
      "Welcome",
      "Welcome"
    ]
  },
  {
    "objectID": "docs-home.html#develop-magasin",
    "href": "docs-home.html#develop-magasin",
    "title": "Magasin documentation",
    "section": "3 Develop magasin",
    "text": "3 Develop magasin\n\nContributing. For developers and contributors.",
    "crumbs": [
      "Home",
      "Welcome",
      "Welcome"
    ]
  },
  {
    "objectID": "install/setup-kubernetes.html",
    "href": "install/setup-kubernetes.html",
    "title": "Setup kubernetes",
    "section": "",
    "text": "Magasin components are designed for installation within Kubernetes clusters. Major cloud providers offer the ability to set up these clusters, but for exploration and testing, you can create one on your desktop.\nIf you already have a Kubernetes cluster, you may skip this step.\nFor setting up a Magasin instance in a local cluster on a desktop, it is advisable to have a minimum of 32 GB of RAM.",
    "crumbs": [
      "Home",
      "Install",
      "Setup kubernetes"
    ]
  },
  {
    "objectID": "install/setup-kubernetes.html#option-1-local-cluster-using-docker-desktop",
    "href": "install/setup-kubernetes.html#option-1-local-cluster-using-docker-desktop",
    "title": "Setup kubernetes",
    "section": "1.1 Option 1: Local cluster using Docker-desktop",
    "text": "1.1 Option 1: Local cluster using Docker-desktop\nIn case you do not have a Kubernetes cluster, another easy way to set it up is through Docker Desktop. It is available for:\n\nGNU/Linux\nMac OS X\nWindows\n\nOnce installed. In Settings / Kubernetes , enable Kubernetes. It will automatically install\n\n\n\nScreenshot of Docker Desktop Kubernetes Settings that allows to enable Kubernetes\n\n\nLastly, on a command line, create the new cluster and use it:\nkubectl config set-context magasin --namespace default --cluster docker-desktop --user=docker-desktop\nkubectl config use-context magasin\nTo ensure that the kubernetes cluster is the correct one check if the name corresponds to the\nkubectl get nodes\nNAME             STATUS   ROLES           AGE   VERSION\ndocker-desktop   Ready    control-plane   48m   v1.28.2\nkubectl get namespaces\nNAME              STATUS   AGE\ndefault           Active   49m\nkube-node-lease   Active   49m\nkube-public       Active   49m\nkube-system       Active   49m\nMore information:\n\nhttps://docs.docker.com/desktop/kubernetes/\nhttps://birthday.play-with-docker.com/kubernetes-docker-desktop/\n\n\n1.1.1 Docker-Desktop resource settings\nGiven that magasin deploys several components, it requires more memory than a regular containerized web application. Because of this, you may need to update the dhe default Docker resources setup.\nGo to Settings / Resources section. Then give Docker as much memory and CPU and memory as you can (see the image below). The minimum recommended is 14GB.\nThe minimum system to run magasin should have 16GB of RAM, though 32GB is recommended.\n\n\n\nResource settings in Docker",
    "crumbs": [
      "Home",
      "Install",
      "Setup kubernetes"
    ]
  },
  {
    "objectID": "install/setup-kubernetes.html#option-2-local-cluster-using-minikube",
    "href": "install/setup-kubernetes.html#option-2-local-cluster-using-minikube",
    "title": "Setup kubernetes",
    "section": "1.2 Option 2: Local cluster using Minikube",
    "text": "1.2 Option 2: Local cluster using Minikube\nIf you don’t have a kubernetes cluster, for testing purposes, you can easily install minikube on your desktop. Minikube is a local Kubernetes cluster created for practicing and learning purposes.\nThe full installation details are described in https://minikube.sigs.k8s.io/docs/start/.\n# GNU/Linux Debian like amd64 / x86 platform\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb\nsudo dpkg -i minikube_latest_amd64.deb\n# Mac OS\nbrew install minikube\nOnce minikube is installed, please make sure to run:\nminikube start\nIf your system does not have the command kubectl already installed, it is also recommended to add the kubeclt alias\nMinikube is installed on top of Docker. So you still need to tweak the resource settings of docker desktop.\n\n\n\n\n\n\nNote\n\n\n\nNote that both of the two options above are just recommended for playing around without needing to deploy any infrastructure, but not for setting up an actual “shared” instance of magasin.",
    "crumbs": [
      "Home",
      "Install",
      "Setup kubernetes"
    ]
  },
  {
    "objectID": "install/manual-installation.html",
    "href": "install/manual-installation.html",
    "title": "Manual installation",
    "section": "",
    "text": "If for some reason the installation scripts do not work for you, or your system is not covered by them, you can install all requirements manually.\nThis installation assumes you already have a Kubernetes cluster. You can follow this guide to setup a cluster.",
    "crumbs": [
      "Home",
      "Install",
      "Manual installation"
    ]
  },
  {
    "objectID": "install/manual-installation.html#install-tools-on-your-computer",
    "href": "install/manual-installation.html#install-tools-on-your-computer",
    "title": "Manual installation",
    "section": "1 Install tools on your computer",
    "text": "1 Install tools on your computer\n\nInstall kubectl\nInstall helm\nInstall Python/pip\n\nVerify kubectl is pointing to the right Kubernetes cluster\nkubectl cluster-info\nIf you are using docker kubernetes cluster, it may display something like:\nKubernetes control plane is running at https://kubernetes.docker.internal:6443\nCoreDNS is running at https://kubernetes.docker.internal:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\nAn Azure Kubernetes Service (AKS) cluster, it may display something like:\n kubectl cluster-info\nKubernetes control plane is running at https://aks-asdfasdf.hcp.westeurope.azmk8s.io:443\nCoreDNS is running at https://aks-asdfasdf.hcp.westeurope.azmk8s.io:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://aks-asdfasdf.hcp.westeurope.azmk8s.io:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.",
    "crumbs": [
      "Home",
      "Install",
      "Manual installation"
    ]
  },
  {
    "objectID": "install/manual-installation.html#deploy-magasin-to-your-cluster",
    "href": "install/manual-installation.html#deploy-magasin-to-your-cluster",
    "title": "Manual installation",
    "section": "2 Deploy magasin to your cluster",
    "text": "2 Deploy magasin to your cluster\nFirst, sdd the magasin helm repo:\nhelm repo add magasin https://unicef.github.io/magasin/\n\"magasin\" has been added to your repositories\nAnd update it:\nhelm repo update magasin\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"magasin\" chart repository\nUpdate Complete. ⎈Happy Helming!⎈\nNow, install the different helm charts (dagster, drill, superset, daskhub …). Run each of the following lines one at a time:\n# helm install &lt;component&gt; magasin/&lt;component&gt; --namespace magasin-&lt;component&gt; --create-namespace\nhelm install dagster magasin/dagster --namespace magasin-dagster --create-namespace\nhelm install drill magasin/drill --namespace magasin-drill --create-namespace\nhelm install superset magasin/superset --namespace magasin-superset --create-namespace\nhelm install daskhub magasin/daskhub --namespace magasin-daskhub --create-namespace\nhelm install operator magasin/operator --namespace magasin-operator --create-namespace\nhelm install tenant magasin/tenant --namespace magasin-tenant --create-namespace\nVerify all the pods in the magasin-* namespaces are in status Running or Completed. Note that, it may take a few minutes before all the items change to that status.\nkubectl get pods --all-namespaces\nNAMESPACE          NAME                                                              READY   STATUS      RESTARTS       AGE\ndefault            net-utils                                                         0/1     Completed   0              55d\nkube-system        coredns-5dd5756b68-fj7bj                                          1/1     Running     16 (12d ago)   90d\nkube-system        coredns-5dd5756b68-qbjf4                                          1/1     Running     16 (12d ago)   90d\nkube-system        etcd-docker-desktop                                               1/1     Running     16 (12d ago)   90d\nkube-system        kube-apiserver-docker-desktop                                     1/1     Running     18 (12d ago)   90d\nkube-system        kube-controller-manager-docker-desktop                            1/1     Running     17 (12d ago)   90d\nkube-system        kube-proxy-n8wwq                                                  1/1     Running     16 (12d ago)   90d\nkube-system        kube-scheduler-docker-desktop                                     1/1     Running     36 (12d ago)   90d\nkube-system        storage-provisioner                                               1/1     Running     49 (12d ago)   90d\nkube-system        vpnkit-controller                                                 1/1     Running     16 (12d ago)   90d\nmagasin-dagster    dagster-daemon-6c99ff9787-xxr8q                                   1/1     Running     0              2m57s\nmagasin-dagster    dagster-dagster-user-deployments-k8s-example-user-code-1-dzw689   1/1     Running     0              2m57s\nmagasin-dagster    dagster-dagster-webserver-79db886f74-vkxvc                        1/1     Running     0              2m57s\nmagasin-dagster    dagster-postgresql-0                                              1/1     Running     0              2m57s\nmagasin-daskhub    api-daskhub-dask-gateway-6b7bf7ff6b-58trn                         1/1     Running     0              53s\nmagasin-daskhub    continuous-image-puller-hn98t                                     1/1     Running     0              53s\nmagasin-daskhub    controller-daskhub-dask-gateway-7f4d8b9475-wsmvl                  1/1     Running     0              53s\nmagasin-daskhub    hub-6bbfd8798d-tbmz9                                              1/1     Running     0              52s\nmagasin-daskhub    proxy-84bdc7766d-89wwd                                            1/1     Running     0              53s\nmagasin-daskhub    traefik-daskhub-dask-gateway-6555db458-rfhpw                      1/1     Running     0              53s\nmagasin-daskhub    user-scheduler-5d8967fc5f-t54p5                                   1/1     Running     0              53s\nmagasin-daskhub    user-scheduler-5d8967fc5f-z9grl                                   1/1     Running     0              52s\nmagasin-drill      drillbit-0                                                        1/1     Running     0              117s\nmagasin-drill      zk-0                                                              1/1     Running     0              117s\nmagasin-operator   console-654bf548c-lfx9p                                           1/1     Running     0              31s\nmagasin-operator   minio-operator-7496fbc5d9-8f56c                                   1/1     Running     0              31s\nmagasin-operator   minio-operator-7496fbc5d9-btk6f                                   1/1     Running     0              31s\nmagasin-superset   superset-7c4f9cb48-zghx2                                          1/1     Running     0              96s\nmagasin-superset   superset-init-db-smj5c                                            0/1     Completed   0              95s\nmagasin-superset   superset-postgresql-0                                             1/1     Running     0              96s\nmagasin-superset   superset-redis-master-0                                           1/1     Running     0              96s\nmagasin-superset   superset-worker-84994b76c4-9gbft                                  1/1     Running     0              96s\nmagasin-tenant     myminio-pool-0-0                                                  2/2     Running     0              19s\nmagasin-tenant     myminio-pool-0-1                                                  2/2     Running     0              19s\n\n2.1 Customize the setup\nIf you have the need to customize your setup through modifying the default values.yaml you can use the -f option.\nFor example, you can create a drill.yaml file with these contents:\n\n\ndrill.yaml\n\n# drill.yaml, which will launch drill with only 2 drillbit (instead of one, the default)\ndrill:\n  count: 2\n\nAnd, then proceed installing the chart:\nhelm install drill magasin/drill --namespace magasin-drill --create-namespace -f drill.yaml\nIf you already installed the chart, you can use helm upgrade instead of helm install:\nhelm upgrade drill magasin/drill --namespace magasin-drill --create-namespace -f drill.yaml\nIf it suceeds, it will update the REVISION number below.\nRelease \"drill\" has been upgraded. Happy Helming!\nNAME: drill\nLAST DEPLOYED: Tue Mar 12 14:24:17 2024\nNAMESPACE: magasin-drill\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\nNOTES:\nTo open the Drill UI\n\n    kubectl port-forward --namespace &lt;namespace&gt; service/drill-service 8047:8047\n\nLaunch a browser at http://localhost:8047\n\nMore details at: https://github.com/unicef/magasin-drill\nIf there is any issue, please check the troubleshooting page.",
    "crumbs": [
      "Home",
      "Install",
      "Manual installation"
    ]
  },
  {
    "objectID": "install/manual-installation.html#whats-next",
    "href": "install/manual-installation.html#whats-next",
    "title": "Manual installation",
    "section": "3 What’s next?",
    "text": "3 What’s next?\nCongratulations, now you have a magasin instance running on your kubernetes cluster. This is just the beggining of an interesting journey. Now you can:\n\nTake a look at magasin tutorial and start using magasin\nLearn about magasin’s architecture. Understand the different components and the underlying technology.\nLearn more about advanced installation. The steps showed in this page were the standard ways of installing magasin, but you can customize much more your setup.",
    "crumbs": [
      "Home",
      "Install",
      "Manual installation"
    ]
  },
  {
    "objectID": "install/uninstall.html",
    "href": "install/uninstall.html",
    "title": "Uninstall magasin",
    "section": "",
    "text": "You can uninstall magasin either by using the uninstall script script or manually.",
    "crumbs": [
      "Home",
      "Install",
      "Uninstall magasin"
    ]
  },
  {
    "objectID": "install/uninstall.html#uninstall-magasin-using-the-script-beta",
    "href": "install/uninstall.html#uninstall-magasin-using-the-script-beta",
    "title": "Uninstall magasin",
    "section": "0.1 Uninstall magasin using the script (beta)",
    "text": "0.1 Uninstall magasin using the script (beta)\nA simple way of getting rid of an instance of magasin within a kubernetes cluster is to use the uninstaller.\nFirst, verify kubectl is pointing to the target cluster by running the command kubectl cluster-info. Take a look at this link if it is not pointing to the right cluster. Then, proceed to run the uninstaller:\n\nUninstall script using a computer with Debian like GNU/Linux:\ncurl -sSL https://unicef.github.io/magasin/uninstall-magasin.sh | bash\nUninstall script using a Mac OS computer:\ncurl -sSL https://unicef.github.io/magasin/uninstall-magasin.sh | zsh\nUninstall script using a Windows computer:\nUse the manual uninstall.",
    "crumbs": [
      "Home",
      "Install",
      "Uninstall magasin"
    ]
  },
  {
    "objectID": "install/uninstall.html#advanced-use-of-uninstall-magasin.sh",
    "href": "install/uninstall.html#advanced-use-of-uninstall-magasin.sh",
    "title": "Uninstall magasin",
    "section": "0.2 Advanced use of uninstall-magasin.sh",
    "text": "0.2 Advanced use of uninstall-magasin.sh\nYou can obtain the list of options by adding the -h option\n./uninstall-magasin.sh -h\nUsage: uninstall-magasin.sh [-c] [-r realm_prefix-realm_postfix (magasin)] [-d] [-h]\n\nThis script uninstall all magasin components from a kubernetes cluster\n\nOptions:\n  -y  Skip prompting questions during uninstall.\n  -c  Only check if all pre-requisites are installed in the local machine.\n  -r  Realm prefix and suffix (default: magasin). Prefix and suffix are separated by '-'.\n        If more than one '-', the last one will be used as separator.\n        The realm 'magasin-new-dev' will set 'magasin-new' as prefix and 'dev' as suffix.\n  -d  Enable debug mode (displays all commands run).\n  -h  Display this help message and exit.\nExamples:\n\nCheck if your computer has all the pre-requisites to run the uninstaller (namely kubectl and helm).\nuninstall-magasin.sh -c\nUninstall magasin from a realm that is different from the standard one\nuninstall-magasin.sh -r magasin-dev",
    "crumbs": [
      "Home",
      "Install",
      "Uninstall magasin"
    ]
  },
  {
    "objectID": "admin-guides/mag-cli.html",
    "href": "admin-guides/mag-cli.html",
    "title": "mag CLI",
    "section": "",
    "text": "1 TODO"
  },
  {
    "objectID": "admin-guides/kubernetes.html",
    "href": "admin-guides/kubernetes.html",
    "title": "Kubernetes guide",
    "section": "",
    "text": "A quick guide for a magasin cluster operator."
  },
  {
    "objectID": "admin-guides/kubernetes.html#general-kubernetes",
    "href": "admin-guides/kubernetes.html#general-kubernetes",
    "title": "Kubernetes guide",
    "section": "0.1 General kubernetes",
    "text": "0.1 General kubernetes\nMagasin is installed in a kubernetes cluster, in this chapter you have a summary of common kubernetes commands.\n\n0.1.1 Get the list of namespaces.\nKubernetes resources are kept in namespaces. A namespace is like a folder but where you can keep kubernetes resources such as pods, secrets, etc.\nkubectl get namespaces\n\n\n0.1.2 Get the list of resources\nTo get the resources available within a particular namespace:\nkubectl get &lt;resource-type&gt; --namespace &lt;namespace&gt;\nwhere resource-type is one of\n\npod,\nsecret,\nservice,\npv (persistent volume),\npvc (persitstent volume claim),\n\nExamples:\nkubectl get pods --namespace magasin-drill\nkubectl get secrets --namespace magasin-drill\nOr alternatively\nkubectl get pods -n magasin-drill\nIf --namespace is not set, it displays the resources of the default namespace.\n\n\n\n\n\n\nTip: to change the default namespace use the command kubectl config set-context --current --namespace=&lt;namespace&gt; to update set the default namespace so you don’t need to set --namespace &lt;namespace&gt; on each command\n\n\n\n\n\n0.1.3 Get the list of secrets\nA secret is a type of resource in kubernetes that allows you to keep configuration information such as usernames, tokens, database names, etc.\nTo get the list of secrets of a particular namespace\nkubectl get secret --namespace magasin-superset\nIn magasin, these are some secrets that contain interesting information for the admin are (format application / secret-name):\n\nmagasin-dagster- / TODO-XXXX:\nmagasin-drill / drill-storage-plugin-secret: contains the initial setup of the storage.\nmagasin-superset / superset-env: Contains the environment variables of the application.\nmagasin-daskhub / hub-env. Contains the environment variables of the application.\n\nOne secret may contain more than variable. To view the different variables (items) of a particular secret:\nkubectl describe secret superset-env --namespace magasin-superset\nwhere superset-env is the name of the secret."
  }
]